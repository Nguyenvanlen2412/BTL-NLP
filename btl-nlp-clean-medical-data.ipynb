{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14072576,"sourceType":"datasetVersion","datasetId":8957868}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nguynvnln22028281/btl-nlp-clean-medical-data?scriptVersionId=286612567\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install langid sentence-transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:45:52.147823Z","iopub.execute_input":"2025-12-12T03:45:52.148119Z","iopub.status.idle":"2025-12-12T03:47:31.066859Z","shell.execute_reply.started":"2025-12-12T03:45:52.148088Z","shell.execute_reply":"2025-12-12T03:47:31.061571Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: langid in /usr/local/lib/python3.11/dist-packages (1.1.6)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from langid) (1.26.4)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.36.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->langid) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->langid) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->langid) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->langid) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->langid) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->langid) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->langid) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->langid) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->langid) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->langid) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->langid) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->langid) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:47:31.07839Z","iopub.execute_input":"2025-12-12T03:47:31.079001Z","iopub.status.idle":"2025-12-12T03:47:33.003555Z","shell.execute_reply.started":"2025-12-12T03:47:31.078944Z","shell.execute_reply":"2025-12-12T03:47:32.998616Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-dataset/public_test.en.txt\n/kaggle/input/nlp-dataset/train.en.txt\n/kaggle/input/nlp-dataset/train.vi.txt\n/kaggle/input/nlp-dataset/public_test.vi.txt\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom tqdm import tqdm\nimport re\nimport unicodedata\nimport langid\nimport torch\n\n# ENABLE semantic filtering for medical data\nuse_semantic_filter = False\nsimilarity_threshold = 0.70  # Raised for technical content\n\nif use_semantic_filter:\n    from sentence_transformers import SentenceTransformer, util\n    print(\"Loading LaBSE model...\")\n    labse = SentenceTransformer(\"sentence-transformers/LaBSE\")\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    labse = labse.to(device)\n\n# ---------------------------\n# Enhanced Cleaning Functions\n# ---------------------------\n\ndef normalize_text(s):\n    \"\"\"Enhanced normalization with encoding fix\"\"\"\n    # Fix common encoding artifacts\n    s = s.replace(\"â€\", \"—\").replace(\"â€™\", \"'\").replace(\"Â±\", \"±\")\n    s = s.replace(\"Ã\", \"ê\").replace(\"Âµ\", \"µ\")\n    \n    # Remove HTML/XML tags\n    s = re.sub(r\"<[^>]+>\", \"\", s)\n    \n    # Normalize Unicode\n    s = unicodedata.normalize(\"NFC\", s)\n    \n    # Remove zero-width characters\n    s = s.replace(\"\\u200b\", \"\").replace(\"\\ufeff\", \"\")\n    \n    # Fix spacing around punctuation\n    s = re.sub(r'\\s+([.,;:!?])', r'\\1', s)\n    \n    # Collapse whitespace\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    \n    return s\n\ndef is_metadata_line(s):\n    \"\"\"Detect non-content lines (headers, page numbers, etc.)\"\"\"\n    s_lower = s.lower()\n    \n    # Check for common metadata patterns\n    metadata_patterns = [\n        r'^page \\d+$',\n        r'^\\d+\\s*$',  # Just numbers\n        r'^(abstract|introduction|conclusion|references?|methods?):\\s*$',\n        r'^\\w+\\s+\\d{4}$',  # \"December 2021\"\n        r'^volume \\d+',\n        r'^doi:',\n        r'^issn',\n        r'^copyright',\n    ]\n    \n    for pattern in metadata_patterns:\n        if re.match(pattern, s_lower):\n            return True\n    \n    # Too short to be meaningful content\n    if len(s.split()) < 4:\n        return True\n        \n    return False\n\ndef heuristic_bad_pair(en, vi):\n    \"\"\"Enhanced domain-specific filters\"\"\"\n    en_low = en.lower()\n    vi_low = vi.lower()\n    \n    # Known translation errors in this dataset\n    if \"vaginal\" in en_low and any(kw in en_low for kw in [\"ear\", \"otitis\", \"tympanogram\"]):\n        return True\n    \n    # Corrupted spellings\n    if any(bad in en_low for bad in [\"otittis\", \"rhinolaryngology\", \"imumnohistochemistry\"]):\n        return True\n    \n    # Detect if one side is metadata but other isn't\n    if is_metadata_line(en) != is_metadata_line(vi):\n        return True\n    \n    # Both are metadata\n    if is_metadata_line(en) and is_metadata_line(vi):\n        return True\n    \n    # Check for URL/email mismatches\n    en_has_url = bool(re.search(r'https?://|www\\.', en))\n    vi_has_url = bool(re.search(r'https?://|www\\.', vi))\n    if en_has_url != vi_has_url:\n        return True\n    \n    # Detect if one sentence has numbers/stats but other doesn't\n    en_has_nums = bool(re.search(r'\\d+[.,]?\\d*\\s*[%±]', en))\n    vi_has_nums = bool(re.search(r'\\d+[.,]?\\d*\\s*[%±]', vi))\n    # Allow medical texts with stats on one side (could be rephrased)\n    # But reject if one has LOTS of numbers and other has none\n    en_num_count = len(re.findall(r'\\d+', en))\n    vi_num_count = len(re.findall(r'\\d+', vi))\n    if en_num_count > 5 and vi_num_count == 0:\n        return True\n    if vi_num_count > 5 and en_num_count == 0:\n        return True\n    \n    return False\n\ndef length_ratio_bad(en, vi):\n    \"\"\"Stricter length constraints for medical abstracts\"\"\"\n    len_en, len_vi = len(en.split()), len(vi.split())\n    \n    # Minimum length (medical sentences are usually substantial)\n    if len_en < 3 or len_vi < 3:\n        return True\n    \n    # Maximum length (likely concatenated paragraphs)\n    if len_en > 150 or len_vi > 150:\n        return True\n    \n    # Tighter ratio for technical content\n    ratio = len_en / max(len_vi, 1)\n    if ratio > 2 or ratio < 0.5:\n        return True\n    return False\n\ndef language_mismatch(en, vi):\n    \"\"\"Relaxed language detection for medical text\"\"\"\n    # Medical text may be unreliable for langid - use as soft signal\n    try:\n        lang_en, score_en = langid.classify(en)\n        lang_vi, score_vi = langid.classify(vi)\n        \n        # Only reject on very confident misdetections\n        if score_en > 0.9 and lang_en != \"en\":\n            return True\n        if score_vi > 0.9 and lang_vi != \"vi\":\n            return True\n            \n    except Exception:\n        # If langid fails, don't reject\n        pass\n    \n    # Additional heuristic: Vietnamese should have tone marks\n    vietnamese_chars = set(\"áàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệíìỉĩịóòỏõọôốồổỗộơớờởỡợúùủũụưứừửữựýỳỷỹỵđ\")\n    vi_lower = vi.lower()\n    has_vietnamese_chars = any(c in vietnamese_chars for c in vi_lower)\n    \n    # Reject if supposed Vietnamese has zero tone marks (likely English)\n    if len(vi_lower) > 20 and not has_vietnamese_chars:\n        return True\n    \n    return False\n\ndef semantic_bad(en, vi):\n    \"\"\"Compute semantic similarity with batching for speed\"\"\"\n    with torch.no_grad():\n        emb_en = labse.encode(en, convert_to_tensor=True, device=device)\n        emb_vi = labse.encode(vi, convert_to_tensor=True, device=device)\n        score = util.cos_sim(emb_en, emb_vi).item()\n    return score < similarity_threshold\n\n\n# ---------------------------\n# Dataset Class\n# ---------------------------\nclass ParallelTextDataset(Dataset):\n    def __init__(self, src_file, tgt_file):\n        self.data = []\n        seen = set()\n        rejected_length_samples = []\n        \n        # Count total lines\n        with open(src_file, 'r', encoding='utf-8') as f:\n            total_lines = sum(1 for _ in f)\n        \n        stats = {\n            'total': 0,\n            'empty': 0,\n            'heuristic': 0,\n            'length': 0,\n            'language': 0,\n            'semantic': 0,\n            'duplicate': 0,\n            'kept': 0\n        }\n        \n        with open(src_file, 'r', encoding='utf-8') as f1, \\\n             open(tgt_file, 'r', encoding='utf-8') as f2:\n            \n            for src_line, tgt_line in tqdm(\n                zip(f1, f2),\n                total=total_lines,\n                desc=f\"Cleaning {src_file.split('/')[-1]}\"\n            ):\n                stats['total'] += 1\n                src = src_line.strip()\n                tgt = tgt_line.strip()\n                \n                if not src or not tgt:\n                    stats['empty'] += 1\n                    continue\n                \n                # Normalize\n                en = normalize_text(src)\n                vi = normalize_text(tgt)\n                \n                if not en or not vi:\n                    stats['empty'] += 1\n                    continue\n                \n                # Filter pipeline\n                if heuristic_bad_pair(en, vi):\n                    stats['heuristic'] += 1\n                    continue\n                \n                if length_ratio_bad(en, vi):\n                    if len(rejected_length_samples) < 20:  # Collect 20 samples\n                        rejected_length_samples.append({\n                            'en': en,\n                            'vi': vi,\n                            'len_en': len(en.split()),\n                            'len_vi': len(vi.split()),\n                            'ratio': len(en.split()) / max(len(vi.split()), 1)\n                        })\n                    stats['length'] += 1\n                    continue\n                \n                if language_mismatch(en, vi):\n                    stats['language'] += 1\n                    continue\n                \n                if use_semantic_filter and semantic_bad(en, vi):\n                    stats['semantic'] += 1\n                    continue\n                \n                # Deduplicate\n                key = en + \"|||\" + vi\n                if key in seen:\n                    stats['duplicate'] += 1\n                    continue\n                \n                seen.add(key)\n                self.data.append((en, vi))\n                stats['kept'] += 1\n        \n        # Print detailed statistics\n        print(f\"\\n{'='*60}\")\n        print(f\"Dataset: {src_file.split('/')[-1]}\")\n        print(f\"{'='*60}\")\n        print(f\"Total pairs processed:     {stats['total']:>6}\")\n        print(f\"  - Empty/blank:           {stats['empty']:>6} ({stats['empty']/stats['total']*100:>5.1f}%)\")\n        print(f\"  - Heuristic filters:     {stats['heuristic']:>6} ({stats['heuristic']/stats['total']*100:>5.1f}%)\")\n        print(f\"  - Length ratio:          {stats['length']:>6} ({stats['length']/stats['total']*100:>5.1f}%)\")\n        print(f\"  - Language mismatch:     {stats['language']:>6} ({stats['language']/stats['total']*100:>5.1f}%)\")\n        if use_semantic_filter:\n            print(f\"  - Semantic similarity:   {stats['semantic']:>6} ({stats['semantic']/stats['total']*100:>5.1f}%)\")\n        print(f\"  - Duplicates:            {stats['duplicate']:>6} ({stats['duplicate']/stats['total']*100:>5.1f}%)\")\n        print(f\"{'='*60}\")\n        print(f\"CLEAN PAIRS KEPT:          {stats['kept']:>6} ({stats['kept']/stats['total']*100:>5.1f}%)\")\n        print(f\"{'='*60}\\n\")\n\n        print(\"\\n\" + \"=\"*60)\n        print(\"SAMPLE REJECTED PAIRS (Length Ratio):\")\n        print(\"=\"*60)\n        for i, sample in enumerate(rejected_length_samples[:10], 1):\n            print(f\"\\n--- Sample {i} ---\")\n            print(f\"EN ({sample['len_en']} words): {sample['en'][:150]}...\")\n            print(f\"VI ({sample['len_vi']} words): {sample['vi'][:150]}...\")\n            print(f\"Ratio: {sample['ratio']:.2f}\")\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# ---------------------------\n# Usage\n# ---------------------------\ntrain_dataset = ParallelTextDataset(\n    \"/kaggle/input/nlp-dataset/train.en.txt\",\n    \"/kaggle/input/nlp-dataset/train.vi.txt\"\n)\n\ntest_dataset = ParallelTextDataset(\n    \"/kaggle/input/nlp-dataset/public_test.en.txt\",\n    \"/kaggle/input/nlp-dataset/public_test.vi.txt\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T03:47:59.565652Z","iopub.execute_input":"2025-12-12T03:47:59.567852Z"}},"outputs":[{"name":"stderr","text":"Cleaning train.en.txt:  36%|███▋      | 181680/500000 [06:32<11:07, 477.08it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Test it\nfor i in range(10):\n    src, tgt = train_dataset[i]\n    print(f\"Source: {src}\")\n    print(f\"Target: {tgt}\")\n\nprint(len(train_dataset))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\ndef save_to_jsonl(dataset, filename):\n    with open(filename, 'w', encoding='utf-8') as f:\n        for en, vi in dataset.data:\n            json_line = json.dumps({\"en\": en, \"vi\": vi}, ensure_ascii=False)\n            f.write(json_line + '\\n')\n\nsave_to_jsonl(train_dataset, \"train_cleaned.jsonl\")\nsave_to_jsonl(test_dataset, \"test_cleaned.jsonl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ntrain_en_path = \"/kaggle/input/nlp-dataset/train.vi.txt\"\n\ndef check_duplicates(file_path):\n    if not os.path.exists(file_path):\n        print(f\"Error: File not found at {file_path}\")\n        return\n\n    seen_lines = set()\n    duplicate_count = 0\n    total_lines = 0\n    \n    # Open the file and iterate line by line\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            total_lines += 1\n            # Strip whitespace to ensure accurate matching (optional but recommended)\n            clean_line = line.strip()\n            \n            if clean_line in seen_lines:\n                duplicate_count += 1\n            else:\n                seen_lines.add(clean_line)\n\n    # Calculate percentage\n    if total_lines > 0:\n        dup_percent = (duplicate_count / total_lines) * 100\n    else:\n        dup_percent = 0\n\n    print(f\"--- Report for {os.path.basename(file_path)} ---\")\n    print(f\"Total Lines:      {total_lines}\")\n    print(f\"Unique Lines:     {len(seen_lines)}\")\n    print(f\"Duplicate Lines:  {duplicate_count}\")\n    print(f\"Duplicate %:      {dup_percent:.2f}%\")\n    \n    if duplicate_count > 0:\n        print(\"\\nResult: The file contains duplicates.\")\n    else:\n        print(\"\\nResult: No duplicates found.\")\n\n# Run the function\ncheck_duplicates(train_en_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-12T12:35:12.495495Z","iopub.execute_input":"2025-12-12T12:35:12.49582Z","iopub.status.idle":"2025-12-12T12:35:13.546158Z","shell.execute_reply.started":"2025-12-12T12:35:12.495798Z","shell.execute_reply":"2025-12-12T12:35:13.544674Z"}},"outputs":[{"name":"stdout","text":"--- Report for train.vi.txt ---\nTotal Lines:      500000\nUnique Lines:     345687\nDuplicate Lines:  154313\nDuplicate %:      30.86%\n\nResult: The file contains duplicates.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}