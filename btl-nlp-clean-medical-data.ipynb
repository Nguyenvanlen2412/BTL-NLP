{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14072576,"sourceType":"datasetVersion","datasetId":8957868}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nguynvnln22028281/btl-nlp-clean-medical-data?scriptVersionId=290108130\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install langid sentence-transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T07:18:17.681453Z","iopub.execute_input":"2026-01-05T07:18:17.681776Z","iopub.status.idle":"2026-01-05T07:19:55.0029Z","shell.execute_reply.started":"2026-01-05T07:18:17.681745Z","shell.execute_reply":"2026-01-05T07:19:55.001555Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: langid in /usr/local/lib/python3.11/dist-packages (1.1.6)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from langid) (1.26.4)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.36.0)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\nRequirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.15.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->langid) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->langid) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->langid) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->langid) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->langid) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->langid) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->langid) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->langid) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->langid) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->langid) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->langid) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->langid) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-05T07:19:55.006366Z","iopub.execute_input":"2026-01-05T07:19:55.007136Z","iopub.status.idle":"2026-01-05T07:19:55.615078Z","shell.execute_reply.started":"2026-01-05T07:19:55.007097Z","shell.execute_reply":"2026-01-05T07:19:55.614075Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-dataset/public_test.en.txt\n/kaggle/input/nlp-dataset/train.en.txt\n/kaggle/input/nlp-dataset/train.vi.txt\n/kaggle/input/nlp-dataset/public_test.vi.txt\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom tqdm import tqdm\nimport re\nimport unicodedata\nimport langid\nimport torch\n\n# ENABLE semantic filtering for medical data\nuse_semantic_filter = False\nsimilarity_threshold = 0.70  # Raised for technical content\n\nif use_semantic_filter:\n    from sentence_transformers import SentenceTransformer, util\n    print(\"Loading LaBSE model...\")\n    labse = SentenceTransformer(\"sentence-transformers/LaBSE\")\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    labse = labse.to(device)\n\n# ---------------------------\n# Enhanced Cleaning Functions\n# ---------------------------\n\ndef normalize_text(s):\n    \"\"\"Enhanced normalization with encoding fix\"\"\"\n    # Fix common encoding artifacts\n    s = s.replace(\"â€\", \"—\").replace(\"â€™\", \"'\").replace(\"Â±\", \"±\")\n    s = s.replace(\"Ã\", \"ê\").replace(\"Âµ\", \"µ\")\n    \n    # Remove HTML/XML tags\n    s = re.sub(r\"<[^>]+>\", \"\", s)\n    \n    # Normalize Unicode\n    s = unicodedata.normalize(\"NFC\", s)\n    \n    # Remove zero-width characters\n    s = s.replace(\"\\u200b\", \"\").replace(\"\\ufeff\", \"\")\n    \n    # Fix spacing around punctuation\n    s = re.sub(r'\\s+([.,;:!?])', r'\\1', s)\n    \n    # Collapse whitespace\n    s = re.sub(r\"\\s+\", \" \", s).strip()\n    \n    return s\n\ndef is_metadata_line(s):\n    \"\"\"Detect non-content lines (headers, page numbers, etc.)\"\"\"\n    s_lower = s.lower()\n    \n    # Check for common metadata patterns\n    metadata_patterns = [\n        r'^page \\d+$',\n        r'^\\d+\\s*$',  # Just numbers\n        r'^(abstract|introduction|conclusion|references?|methods?):\\s*$',\n        r'^\\w+\\s+\\d{4}$',  # \"December 2021\"\n        r'^volume \\d+',\n        r'^doi:',\n        r'^issn',\n        r'^copyright',\n    ]\n    \n    for pattern in metadata_patterns:\n        if re.match(pattern, s_lower):\n            return True\n    \n    # Too short to be meaningful content\n    if len(s.split()) < 4:\n        return True\n        \n    return False\n\ndef heuristic_bad_pair(en, vi):\n    \"\"\"Enhanced domain-specific filters\"\"\"\n    en_low = en.lower()\n    vi_low = vi.lower()\n    \n    # Known translation errors in this dataset\n    if \"vaginal\" in en_low and any(kw in en_low for kw in [\"ear\", \"otitis\", \"tympanogram\"]):\n        return True\n    \n    # Corrupted spellings\n    if any(bad in en_low for bad in [\"otittis\", \"rhinolaryngology\", \"imumnohistochemistry\"]):\n        return True\n    \n    # Detect if one side is metadata but other isn't\n    if is_metadata_line(en) != is_metadata_line(vi):\n        return True\n    \n    # Both are metadata\n    if is_metadata_line(en) and is_metadata_line(vi):\n        return True\n    \n    # Check for URL/email mismatches\n    en_has_url = bool(re.search(r'https?://|www\\.', en))\n    vi_has_url = bool(re.search(r'https?://|www\\.', vi))\n    if en_has_url != vi_has_url:\n        return True\n    \n    # Detect if one sentence has numbers/stats but other doesn't\n    en_has_nums = bool(re.search(r'\\d+[.,]?\\d*\\s*[%±]', en))\n    vi_has_nums = bool(re.search(r'\\d+[.,]?\\d*\\s*[%±]', vi))\n    # Allow medical texts with stats on one side (could be rephrased)\n    # But reject if one has LOTS of numbers and other has none\n    en_num_count = len(re.findall(r'\\d+', en))\n    vi_num_count = len(re.findall(r'\\d+', vi))\n    if en_num_count > 5 and vi_num_count == 0:\n        return True\n    if vi_num_count > 5 and en_num_count == 0:\n        return True\n    \n    return False\n\ndef length_ratio_bad(en, vi):\n    \"\"\"Stricter length constraints for medical abstracts\"\"\"\n    len_en, len_vi = len(en.split()), len(vi.split())\n    \n    # Minimum length (medical sentences are usually substantial)\n    if len_en < 3 or len_vi < 3:\n        return True\n    \n    # Maximum length (likely concatenated paragraphs)\n    if len_en > 150 or len_vi > 150:\n        return True\n    \n    # Tighter ratio for technical content\n    ratio = len_en / max(len_vi, 1)\n    if ratio > 2 or ratio < 0.5:\n        return True\n    return False\n\ndef language_mismatch(en, vi):\n    \"\"\"Relaxed language detection for medical text\"\"\"\n    # Medical text may be unreliable for langid - use as soft signal\n    try:\n        lang_en, score_en = langid.classify(en)\n        lang_vi, score_vi = langid.classify(vi)\n        \n        # Only reject on very confident misdetections\n        if score_en > 0.9 and lang_en != \"en\":\n            return True\n        if score_vi > 0.9 and lang_vi != \"vi\":\n            return True\n            \n    except Exception:\n        # If langid fails, don't reject\n        pass\n    \n    # Additional heuristic: Vietnamese should have tone marks\n    vietnamese_chars = set(\"áàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệíìỉĩịóòỏõọôốồổỗộơớờởỡợúùủũụưứừửữựýỳỷỹỵđ\")\n    vi_lower = vi.lower()\n    has_vietnamese_chars = any(c in vietnamese_chars for c in vi_lower)\n    \n    # Reject if supposed Vietnamese has zero tone marks (likely English)\n    if len(vi_lower) > 20 and not has_vietnamese_chars:\n        return True\n    \n    return False\n\ndef semantic_bad(en, vi):\n    \"\"\"Compute semantic similarity with batching for speed\"\"\"\n    with torch.no_grad():\n        emb_en = labse.encode(en, convert_to_tensor=True, device=device)\n        emb_vi = labse.encode(vi, convert_to_tensor=True, device=device)\n        score = util.cos_sim(emb_en, emb_vi).item()\n    return score < similarity_threshold\n\n\n# ---------------------------\n# Dataset Class\n# ---------------------------\nclass ParallelTextDataset(Dataset):\n    def __init__(self, src_file, tgt_file):\n        self.data = []\n        seen = set()\n        rejected_length_samples = []\n        \n        # Count total lines\n        with open(src_file, 'r', encoding='utf-8') as f:\n            total_lines = sum(1 for _ in f)\n        \n        stats = {\n            'total': 0,\n            'empty': 0,\n            'heuristic': 0,\n            'length': 0,\n            'language': 0,\n            'semantic': 0,\n            'duplicate': 0,\n            'kept': 0\n        }\n        \n        with open(src_file, 'r', encoding='utf-8') as f1, \\\n             open(tgt_file, 'r', encoding='utf-8') as f2:\n            \n            for src_line, tgt_line in tqdm(\n                zip(f1, f2),\n                total=total_lines,\n                desc=f\"Cleaning {src_file.split('/')[-1]}\"\n            ):\n                stats['total'] += 1\n                src = src_line.strip()\n                tgt = tgt_line.strip()\n                \n                if not src or not tgt:\n                    stats['empty'] += 1\n                    continue\n                \n                # Normalize\n                en = normalize_text(src)\n                vi = normalize_text(tgt)\n                \n                if not en or not vi:\n                    stats['empty'] += 1\n                    continue\n                \n                # Filter pipeline\n                if heuristic_bad_pair(en, vi):\n                    stats['heuristic'] += 1\n                    continue\n                \n                if length_ratio_bad(en, vi):\n                    if len(rejected_length_samples) < 20:  # Collect 20 samples\n                        rejected_length_samples.append({\n                            'en': en,\n                            'vi': vi,\n                            'len_en': len(en.split()),\n                            'len_vi': len(vi.split()),\n                            'ratio': len(en.split()) / max(len(vi.split()), 1)\n                        })\n                    stats['length'] += 1\n                    continue\n                \n                if language_mismatch(en, vi):\n                    stats['language'] += 1\n                    continue\n                \n                if use_semantic_filter and semantic_bad(en, vi):\n                    stats['semantic'] += 1\n                    continue\n                \n                # Deduplicate\n                key = en + \"|||\" + vi\n                if key in seen:\n                    stats['duplicate'] += 1\n                    continue\n                \n                seen.add(key)\n                self.data.append((en, vi))\n                stats['kept'] += 1\n        \n        # Print detailed statistics\n        print(f\"\\n{'='*60}\")\n        print(f\"Dataset: {src_file.split('/')[-1]}\")\n        print(f\"{'='*60}\")\n        print(f\"Total pairs processed:     {stats['total']:>6}\")\n        print(f\"  - Empty/blank:           {stats['empty']:>6} ({stats['empty']/stats['total']*100:>5.1f}%)\")\n        print(f\"  - Heuristic filters:     {stats['heuristic']:>6} ({stats['heuristic']/stats['total']*100:>5.1f}%)\")\n        print(f\"  - Length ratio:          {stats['length']:>6} ({stats['length']/stats['total']*100:>5.1f}%)\")\n        print(f\"  - Language mismatch:     {stats['language']:>6} ({stats['language']/stats['total']*100:>5.1f}%)\")\n        if use_semantic_filter:\n            print(f\"  - Semantic similarity:   {stats['semantic']:>6} ({stats['semantic']/stats['total']*100:>5.1f}%)\")\n        print(f\"  - Duplicates:            {stats['duplicate']:>6} ({stats['duplicate']/stats['total']*100:>5.1f}%)\")\n        print(f\"{'='*60}\")\n        print(f\"CLEAN PAIRS KEPT:          {stats['kept']:>6} ({stats['kept']/stats['total']*100:>5.1f}%)\")\n        print(f\"{'='*60}\\n\")\n\n        print(\"\\n\" + \"=\"*60)\n        print(\"SAMPLE REJECTED PAIRS (Length Ratio):\")\n        print(\"=\"*60)\n        for i, sample in enumerate(rejected_length_samples[:10], 1):\n            print(f\"\\n--- Sample {i} ---\")\n            print(f\"EN ({sample['len_en']} words): {sample['en'][:150]}...\")\n            print(f\"VI ({sample['len_vi']} words): {sample['vi'][:150]}...\")\n            print(f\"Ratio: {sample['ratio']:.2f}\")\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# ---------------------------\n# Usage\n# ---------------------------\ntrain_dataset = ParallelTextDataset(\n    \"/kaggle/input/nlp-dataset/train.en.txt\",\n    \"/kaggle/input/nlp-dataset/train.vi.txt\"\n)\n\ntest_dataset = ParallelTextDataset(\n    \"/kaggle/input/nlp-dataset/public_test.en.txt\",\n    \"/kaggle/input/nlp-dataset/public_test.vi.txt\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T07:19:55.619145Z","iopub.execute_input":"2026-01-05T07:19:55.619497Z","iopub.status.idle":"2026-01-05T07:40:07.422047Z","shell.execute_reply.started":"2026-01-05T07:19:55.619468Z","shell.execute_reply":"2026-01-05T07:40:07.421015Z"}},"outputs":[{"name":"stderr","text":"Cleaning train.en.txt: 100%|██████████| 500000/500000 [19:58<00:00, 417.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nDataset: train.en.txt\n============================================================\nTotal pairs processed:     500000\n  - Empty/blank:                0 (  0.0%)\n  - Heuristic filters:       4545 (  0.9%)\n  - Length ratio:           32286 (  6.5%)\n  - Language mismatch:        549 (  0.1%)\n  - Duplicates:            140575 ( 28.1%)\n============================================================\nCLEAN PAIRS KEPT:          322045 ( 64.4%)\n============================================================\n\n\n============================================================\nSAMPLE REJECTED PAIRS (Length Ratio):\n============================================================\n\n--- Sample 1 ---\nEN (8 words): Typical endoscopy are concave eardrum, yellow or bubbly....\nVI (18 words): Hình ảnh nội soi màng nhĩ điển hình là màng nhĩ lõm, màu vàng hoặc có bóng khí....\nRatio: 0.44\n\n--- Sample 2 ---\nEN (15 words): To evaluate clinical, laboratory features and relationship with bone metastasis among hepatocellular carcinoma (HCC) patients....\nVI (34 words): Mô tả một số đặc điểm lâm sàng, cận lâm sàng, hình ảnh xạ hình xương và mối liên quan với di căn xương ở bệnh nhân ung thư biểu mô tế bào gan....\nRatio: 0.44\n\n--- Sample 3 ---\nEN (7 words): Thyroid storm in a patient with trauma...\nVI (27 words): Nhân một trường hợp cơn bão giáp ở bệnh nhân đa chấn thương được chẩn đoán và điều trị thành công tại Bệnh viện Quân y 175...\nRatio: 0.26\n\n--- Sample 4 ---\nEN (13 words): Formulate various porous microsphere batches with different particle mean sizes and encapsulation ratios....\nVI (31 words): Thiết lập các thông số điều chế phù hợp để tạo ra các vi cầu xốp chứa dược chất metronidazol có đặc tính kích thước và tỉ lệ tải khác nhau....\nRatio: 0.42\n\n--- Sample 5 ---\nEN (17 words): Independent risk factors contributing to the increased in mortality were immunosuppression, invasive procedures, and multidrug-resistant bacterial inf...\nVI (35 words): Các yếu tố nguy cơ độc lập góp phần tăng tỷ lệ tử vong trong đơn vị HSTC là suy giảm miễn dịch, các thủ thuật xâm lấn và nhiễm vi khuẩn đa kháng thuốc...\nRatio: 0.49\n\n--- Sample 6 ---\nEN (21 words): Methods: Conducting a systematic literature review on cost-effectiveness, QALYs,... based on Pubmed, Embase and Cochrane database,... from 2000 to Aug...\nVI (48 words): Phương pháp nghiên cứu: Tiến hành tổng quan tài liệu một các có hệ thống để đánh giá CPHQ của liệu pháp nhắm trúng đích trong điều trị UTPKTBN dựa trê...\nRatio: 0.44\n\n--- Sample 7 ---\nEN (5 words): Studies used the CHEERS checklist....\nVI (16 words): Đánh giá chất lượng của các nghiên cứu Kinh tế Y tế bằng bảng kiểm CHEERS....\nRatio: 0.31\n\n--- Sample 8 ---\nEN (21 words): 18F-FDG cardiac PET / CT was evaluated to assess myocardial viability and coronary angiography was done for patients who had indication....\nVI (43 words): Sau đó, tiến hành chụp PET / CT sử dụng 18 F-FDG đánh giá cơ tim còn sống cho những BN có kết quả là khuyết xạ cố định trên XHTMCT và chụp động mạch v...\nRatio: 0.49\n\n--- Sample 9 ---\nEN (10 words): Methods: The interventional study design was applied in this study....\nVI (21 words): Đối tượng và phương pháp nghiên cứu: Nghiên cứu can thiệp, thiết kế so sánh trước sau và có đối chứng....\nRatio: 0.48\n\n--- Sample 10 ---\nEN (18 words): All of healthcare staffs and health workers in 4 communes of Daitu districts were recruited in this study....\nVI (39 words): Chọn mẫu chủ đích toàn bộ nhân viên y tế xã (cán bộ y tế và y tế thôn bản) của 4 xã Hoàng Nông, Bản Ngoại, Khôi Kỳ và Bình Thuận tại Huyện Đại Từ, tỉn...\nRatio: 0.46\n","output_type":"stream"},{"name":"stderr","text":"Cleaning public_test.en.txt: 100%|██████████| 3000/3000 [00:07<00:00, 423.90it/s]","output_type":"stream"},{"name":"stdout","text":"\n============================================================\nDataset: public_test.en.txt\n============================================================\nTotal pairs processed:       3000\n  - Empty/blank:                0 (  0.0%)\n  - Heuristic filters:         16 (  0.5%)\n  - Length ratio:             188 (  6.3%)\n  - Language mismatch:          0 (  0.0%)\n  - Duplicates:                 3 (  0.1%)\n============================================================\nCLEAN PAIRS KEPT:            2793 ( 93.1%)\n============================================================\n\n\n============================================================\nSAMPLE REJECTED PAIRS (Length Ratio):\n============================================================\n\n--- Sample 1 ---\nEN (18 words): Knowledge, practices in public health service utilization among health insurance card’s holders and influencing factors in Vientiane, Lao...\nVI (45 words): Thực trạng kiến thức và thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố ảnh hưởng...\nRatio: 0.40\n\n--- Sample 2 ---\nEN (21 words): Describe knowledge, practices in public health service utilization among health insurance card's holders and influencing factors in Vientiane, Lao PDR...\nVI (51 words): Mô tả thực trạng kiến thức, thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố liên ...\nRatio: 0.41\n\n--- Sample 3 ---\nEN (7 words): Unreasonable prescriptions in PPI used mainly gastritis....\nVI (16 words): Đơn thuốc chưa hợp lý trong sử dụng PPI chủ yếu là bệnh viêm dạ dày....\nRatio: 0.44\n\n--- Sample 4 ---\nEN (5 words): C-met expression in gastric adenocarcinoma...\nVI (16 words): Sự biểu lộ của C-met trong ung thư biểu mô tuyến dạ dày vùng hang vị...\nRatio: 0.31\n\n--- Sample 5 ---\nEN (14 words): To report the clinical and radiological outcomes after corrective osteotomy in ankylosing spondylitis patients....\nVI (38 words): Đánh giá kết quả về lâm sàng và hình ảnh học X-quang sau điều trị phẫu thuật cắt chêm xương đốt sống để nắn chỉnh gù toàn bộ cột sống do bệnh viêm cột...\nRatio: 0.37\n\n--- Sample 6 ---\nEN (9 words): There were 28 surgery-related complications in 12 patients (17.3%)....\nVI (25 words): Ghi nhận 28 biến chứng liên quan đến phẫu thuật xảy ra ở 12 bệnh nhân (chiếm 17,4%) và không có trường hợp tử vong....\nRatio: 0.36\n\n--- Sample 7 ---\nEN (18 words): Subject and method: We prospectively analyzed 69 ankylosing spondylitis patients who underwent corrective osteotomy between 2010 and 2022....\nVI (40 words): Đối tượng và phương pháp: Nghiên cứu tiến cứu, mô tả theo chiều dọc trên 69 bệnh nhân có biểu hiện gù toàn bộ cột sống mức độ nặng được phẫu thuật nắn...\nRatio: 0.45\n\n--- Sample 8 ---\nEN (4 words): Periodontal treatment improves glycosylatedhemoglobin.In...\nVI (23 words): Điều trị nha chu giúp giảm đường huyết và ngược lại, kiểm soát đường huyết tốt có thể cải thiện bệnh nha chu....\nRatio: 0.17\n\n--- Sample 9 ---\nEN (18 words): Materials and Methods: A clinical trials study on 76 type 2 diabetes patients with mild and moderate periodontitis....\nVI (38 words): Đối tượng và phương pháp nghiên cứu: Nghiên cứu thử nghiệm lâm sàng có nhóm chứng trên 76 bệnh nhân đái tháo đường týp 2 có viêm nha chu nhẹ và trung ...\nRatio: 0.47\n\n--- Sample 10 ---\nEN (13 words): Periodontal index and HbA1c were recorded at baseline and six month after treatment....\nVI (28 words): Số liệu thu thập gồm các chỉ số nha chu và giá trị HbA1c được ghi nhận vào lúc khám ban đầu và sau 6 tháng điều trị....\nRatio: 0.46\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Test it\nfor i in range(10):\n    src, tgt = train_dataset[i]\n    print(f\"Source: {src}\")\n    print(f\"Target: {tgt}\")\n\nprint(len(train_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T07:40:07.424261Z","iopub.execute_input":"2026-01-05T07:40:07.425118Z","iopub.status.idle":"2026-01-05T07:40:07.431518Z","shell.execute_reply.started":"2026-01-05T07:40:07.425087Z","shell.execute_reply":"2026-01-05T07:40:07.430414Z"}},"outputs":[{"name":"stdout","text":"Source: There was a relation between vasodilatation and vaginal dysfunction.\nTarget: Có sự liên quan giữa độ quá phát V.a với mức độ rối loạn chức năng vòi nhĩ.\nSource: Main symptoms are rhinitis, nasal congestion, tinnitus.\nTarget: Triệu chứng cơ năng nổi bật là chảy mũi, ngạt mũi và ù tai.\nSource: Method: Prospective descriptive method with clinical intervention.\nTarget: Phương pháp: Nghiên cứu mô tả từng trường hợp có can thiệp.\nSource: Results: Mean age was 5.1.\nTarget: Kết quả: Tuổi trung bình 5,1.\nSource: Gender: male (52.6%), female (47.4%).\nTarget: Tỉ lệ giới: Nam (52,6%), nữ (47,4%).\nSource: The main physical symptoms are tinnitus (52.6%), rhinitis, nasal congestion (65.8%), no symptoms (23.7%).\nTarget: Triệu chứng cơ năng: Ù tai (52,6%), nghe kém (28,9%), chảy mũi, ngạt tắc mũi (65,8%), không có triệu chứng (23,7%).\nSource: Endoscopy: Typical endoscopy are concave eardrum, yellow or bubbly, V a level 3 (55.3%), level 4 (34.2%), level 1 (0%).\nTarget: Thực thể: Hình ảnh nội soi màng nhĩ điển hình là màng nhĩ lõm, màu vàng hoặc có bóng khí, V.a quá phát độ 3 (55,3%), độ 4 (34,2%), độ 1 (0%).\nSource: Type of tympanograms: Type C (61%), Type B (32.2%).\nTarget: Type nhĩ đồ: Type C (61%), type B (32,2%), type As (6,8%).\nSource: Type As (6.8%), Atlasic shape (50.8%), flat (32.2%).\nTarget: Hình dạng nhĩ đồ: Hình đồi (50,8%), phẳng (32,2%).\nSource: PTA average 26.3 ± 15.7.\nTarget: PTA trung bình 26,3 ± 15,7.\n322045\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import json\n\ndef save_to_jsonl(dataset, filename):\n    with open(filename, 'w', encoding='utf-8') as f:\n        for en, vi in dataset.data:\n            json_line = json.dumps({\"en\": en, \"vi\": vi}, ensure_ascii=False)\n            f.write(json_line + '\\n')\n\nsave_to_jsonl(train_dataset, \"train_cleaned.jsonl\")\nsave_to_jsonl(test_dataset, \"test_cleaned.jsonl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T07:40:07.432602Z","iopub.execute_input":"2026-01-05T07:40:07.432903Z","iopub.status.idle":"2026-01-05T07:40:10.661741Z","shell.execute_reply.started":"2026-01-05T07:40:07.432881Z","shell.execute_reply":"2026-01-05T07:40:10.660718Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\n\ntrain_en_path = \"/kaggle/input/nlp-dataset/train.vi.txt\"\n\ndef check_duplicates(file_path):\n    if not os.path.exists(file_path):\n        print(f\"Error: File not found at {file_path}\")\n        return\n\n    seen_lines = set()\n    duplicate_count = 0\n    total_lines = 0\n    \n    # Open the file and iterate line by line\n    with open(file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            total_lines += 1\n            # Strip whitespace to ensure accurate matching (optional but recommended)\n            clean_line = line.strip()\n            \n            if clean_line in seen_lines:\n                duplicate_count += 1\n            else:\n                seen_lines.add(clean_line)\n\n    # Calculate percentage\n    if total_lines > 0:\n        dup_percent = (duplicate_count / total_lines) * 100\n    else:\n        dup_percent = 0\n\n    print(f\"--- Report for {os.path.basename(file_path)} ---\")\n    print(f\"Total Lines:      {total_lines}\")\n    print(f\"Unique Lines:     {len(seen_lines)}\")\n    print(f\"Duplicate Lines:  {duplicate_count}\")\n    print(f\"Duplicate %:      {dup_percent:.2f}%\")\n    \n    if duplicate_count > 0:\n        print(\"\\nResult: The file contains duplicates.\")\n    else:\n        print(\"\\nResult: No duplicates found.\")\n\n# Run the function\ncheck_duplicates(train_en_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}