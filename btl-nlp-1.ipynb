{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e70933f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:44:07.545923Z",
     "iopub.status.busy": "2025-12-14T08:44:07.545683Z",
     "iopub.status.idle": "2025-12-14T08:44:08.813961Z",
     "shell.execute_reply": "2025-12-14T08:44:08.813136Z"
    },
    "papermill": {
     "duration": 1.275084,
     "end_time": "2025-12-14T08:44:08.815300",
     "exception": false,
     "start_time": "2025-12-14T08:44:07.540216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/btl-nlp-cleandata/test_cleaned.jsonl\n",
      "/kaggle/input/btl-nlp-cleandata/train_cleaned.jsonl\n",
      "/kaggle/input/btl-nlp-cleandata/__results__.html\n",
      "/kaggle/input/btl-nlp-cleandata/__notebook__.ipynb\n",
      "/kaggle/input/btl-nlp-cleandata/__output__.json\n",
      "/kaggle/input/btl-nlp-cleandata/custom.css\n",
      "/kaggle/input/btl-nlp-cleandata/.virtual_documents/__notebook_source__.ipynb\n",
      "/kaggle/input/fork-of-btl-nlp-cleandata/test_cleaned.jsonl\n",
      "/kaggle/input/fork-of-btl-nlp-cleandata/train_cleaned.jsonl\n",
      "/kaggle/input/fork-of-btl-nlp-cleandata/__results__.html\n",
      "/kaggle/input/fork-of-btl-nlp-cleandata/__huggingface_repos__.json\n",
      "/kaggle/input/fork-of-btl-nlp-cleandata/__notebook__.ipynb\n",
      "/kaggle/input/fork-of-btl-nlp-cleandata/valid_cleaned.jsonl\n",
      "/kaggle/input/fork-of-btl-nlp-cleandata/__output__.json\n",
      "/kaggle/input/fork-of-btl-nlp-cleandata/custom.css\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d2da931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:44:08.823905Z",
     "iopub.status.busy": "2025-12-14T08:44:08.823597Z",
     "iopub.status.idle": "2025-12-14T08:44:16.306539Z",
     "shell.execute_reply": "2025-12-14T08:44:16.305774Z"
    },
    "papermill": {
     "duration": 7.488366,
     "end_time": "2025-12-14T08:44:16.307782",
     "exception": false,
     "start_time": "2025-12-14T08:44:08.819416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8b200b515b4cdb9605d3555a15d562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2930d62dacb84310b8e633384d47b850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad2ec483a4d4e36bb2a24ea75f59b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Final dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'vi'],\n",
      "        num_rows: 959482\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['en', 'vi'],\n",
      "        num_rows: 11004\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['en', 'vi'],\n",
      "        num_rows: 10917\n",
      "    })\n",
      "})\n",
      "  Train: 959482 pairs\n",
      "  Validation: 11004 pairs\n",
      "  Test: 10917 pairs\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "\n",
    "# Load data\n",
    "data_files = {\n",
    "    \"train\": \"/kaggle/input/fork-of-btl-nlp-cleandata/train_cleaned.jsonl\",\n",
    "    \"val\":\"/kaggle/input/fork-of-btl-nlp-cleandata/valid_cleaned.jsonl\",\n",
    "    \"test\": \"/kaggle/input/fork-of-btl-nlp-cleandata/test_cleaned.jsonl\"\n",
    "}\n",
    "dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "# Reassemble into final dataset structure\n",
    "dataset = DatasetDict({\n",
    "    'train': dataset['train'],\n",
    "    'validation': dataset['val'],  # Note: this is validation, not test\n",
    "    'test': dataset['test']\n",
    "})\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Final dataset structure:\")\n",
    "print(dataset)\n",
    "print(f\"  Train: {len(dataset['train'])} pairs\")\n",
    "print(f\"  Validation: {len(dataset['validation'])} pairs\")\n",
    "print(f\"  Test: {len(dataset['test'])} pairs\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b813740",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:44:16.317288Z",
     "iopub.status.busy": "2025-12-14T08:44:16.316711Z",
     "iopub.status.idle": "2025-12-14T08:44:16.326736Z",
     "shell.execute_reply": "2025-12-14T08:44:16.325969Z"
    },
    "papermill": {
     "duration": 0.015711,
     "end_time": "2025-12-14T08:44:16.327762",
     "exception": false,
     "start_time": "2025-12-14T08:44:16.312051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATASET INSPECTION\n",
      "============================================================\n",
      "\n",
      "TRAIN split:\n",
      "  Total examples: 959482\n",
      "\n",
      "  Example 304084:\n",
      "    EN (15 words): In 1569, Pope Pius V elevated him to the rank of Grand Duke of Tuscany....\n",
      "    VI (17 words): Năm 1569, Qiáo Hoàng Pius V đã nâng ông lên vị trí Đại Công tước xứ Toscana....\n",
      "\n",
      "  Example 101611:\n",
      "    EN (11 words): No body's been found in the harbor or anywhere near it....\n",
      "    VI (17 words): Không có cái xác nào được tìm thấy trong cảng cũng như khu vực quanh đó cả....\n",
      "\n",
      "  Example 784670:\n",
      "    EN (10 words): It appears you've been beaten by the ace of clubs....\n",
      "    VI (11 words): Có vẻ cô đã bị đánh bại... bởi con Xì Chuồn....\n",
      "\n",
      "  Example 468624:\n",
      "    EN (6 words): Since when did you use formalities?...\n",
      "    VI (9 words): Ngươi bắt đầu khách sáo từ khi nào vậy?...\n",
      "\n",
      "  Example 162589:\n",
      "    EN (17 words): You heard him say it. He's killed my father and now you will let him kill me....\n",
      "    VI (19 words): Ông nghe hắn nói rồi đấy, hắn đã giết cha tôi, và giờ ông để cho hắn giết tôi....\n",
      "\n",
      "VALIDATION split:\n",
      "  Total examples: 11004\n",
      "\n",
      "  Example 6619:\n",
      "    EN (29 words): The city was platted in 1862 and was chosen as the county seat two years later, when Grant County wa...\n",
      "    VI (27 words): Thị trấn được thị sát vào năm 1862 và được chọn là quận lỵ hai năm sau đó khi Quận Grant được tách r...\n",
      "\n",
      "  Example 2139:\n",
      "    EN (17 words): There'll be a slight delay in the divorce proceedings... while the couple hides out from the killers...\n",
      "    VI (24 words): Chuyện này sẽ làm chậm trễ quá trình ly hôn của chúng ta. Còn chúng ta lại đang chạy trốn mấy tay sá...\n",
      "\n",
      "  Example 3816:\n",
      "    EN (10 words): Before the Revolution, the people had little power or voice....\n",
      "    VI (14 words): Trước Cách mạng, người dân Pháp có rất ít quyền lực hoặc tiếng nói....\n",
      "\n",
      "  Example 4024:\n",
      "    EN (21 words): It was invented with the intent of allowing engineers to create prototypes of their designs in a mor...\n",
      "    VI (25 words): Nó được phát minh với mục đích cho phép các kỹ sư tạo ra nguyên mẫu thiết kế của họ theo cách hiệu q...\n",
      "\n",
      "  Example 2525:\n",
      "    EN (7 words): Look, I was there for the science....\n",
      "    VI (7 words): Tôi ở đó để làm khoa học....\n",
      "\n",
      "TEST split:\n",
      "  Total examples: 10917\n",
      "\n",
      "  Example 10095:\n",
      "    EN (13 words): I mean, why would you want to do all these things for real?...\n",
      "    VI (16 words): Ý tôi là, tại sao chúng ta muốn làm những điều này trở thành sự thật?...\n",
      "\n",
      "  Example 1792:\n",
      "    EN (4 words): Kronos is waiting Father....\n",
      "    VI (5 words): Kronos đang chờ thưa cha....\n",
      "\n",
      "  Example 10124:\n",
      "    EN (9 words): I might be able to slow them down though....\n",
      "    VI (10 words): Tôi ngĩa có thể làm chúng chậm lại 1 chút....\n",
      "\n",
      "  Example 1157:\n",
      "    EN (6 words): But what do you really do?...\n",
      "    VI (6 words): Nhưng thực sự cô làm gì?...\n",
      "\n",
      "  Example 8831:\n",
      "    EN (4 words): Have you enough money?...\n",
      "    VI (5 words): Cậu có đủ tiền không?...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def inspect_dataset(dataset, num_samples=5):\n",
    "    \"\"\"Inspect random samples from dataset\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATASET INSPECTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Check for None/empty values\n",
    "    for split in dataset.keys():\n",
    "        print(f\"\\n{split.upper()} split:\")\n",
    "        print(f\"  Total examples: {len(dataset[split])}\")\n",
    "        \n",
    "        # Sample random examples\n",
    "        indices = random.sample(range(len(dataset[split])), min(num_samples, len(dataset[split])))\n",
    "        \n",
    "        for idx in indices:\n",
    "            example = dataset[split][idx]\n",
    "            en = example['en']\n",
    "            vi = example['vi']\n",
    "            \n",
    "            print(f\"\\n  Example {idx}:\")\n",
    "            print(f\"    EN ({len(en.split())} words): {en[:100]}...\")\n",
    "            print(f\"    VI ({len(vi.split())} words): {vi[:100]}...\")\n",
    "            \n",
    "            # Sanity checks\n",
    "            if not en or not vi:\n",
    "                print(\"    ⚠️  WARNING: Empty field detected!\")\n",
    "            if len(en.split()) < 3 or len(vi.split()) < 3:\n",
    "                print(\"    ⚠️  WARNING: Very short sentence!\")\n",
    "\n",
    "inspect_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ebff11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:44:16.336780Z",
     "iopub.status.busy": "2025-12-14T08:44:16.336568Z",
     "iopub.status.idle": "2025-12-14T08:46:23.770168Z",
     "shell.execute_reply": "2025-12-14T08:46:23.769309Z"
    },
    "papermill": {
     "duration": 127.439748,
     "end_time": "2025-12-14T08:46:23.771472",
     "exception": false,
     "start_time": "2025-12-14T08:44:16.331724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "        TRAINING EN–VI MEDICAL BYTE-LEVEL TOKENIZER\n",
      "======================================================================\n",
      "\n",
      "1. Preparing training corpus...\n",
      "2. Initializing byte-level BPE tokenizer...\n",
      "3. Configuring BPE trainer...\n",
      "4. Training tokenizer on corpus (vocab=40000)...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5. Saving tokenizer to: medical_envi_tokenizer/\n",
      "\n",
      "6. Testing tokenizer with sample medical text:\n",
      "\n",
      "Text: Với cả hai phương pháp, không có hai bản in nào giống bản nào, nhưng cả hai đều cho ra hình ảnh ấn tượng.\n",
      "Tokens: ['vá»Ľi', 'Ġcáº£', 'Ġhai', 'ĠphÆ°Æ¡ng', 'ĠphÃ¡p', ',', 'ĠkhÃ´ng', 'ĠcÃ³', 'Ġhai', 'Ġbáº£n', 'Ġin', 'ĠnÃło', 'Ġgiá»ĳng', 'Ġbáº£n', 'ĠnÃło', ',', 'ĠnhÆ°ng', 'Ġcáº£', 'Ġhai', 'ĠÄĳá»ģu', 'Ġcho', 'Ġra', 'ĠhÃ¬nh', 'Ġáº£nh', 'Ġáº¥n', 'ĠtÆ°á»£ng', '.']\n",
      "IDs: [2, 10373, 550, 803, 1395, 1132, 17, 385, 362, 803, 802, 316, 784, 1356, 802, 784, 17, 629, 550, 803, 1421, 433, 487, 907, 1291, 2178, 1811, 19, 3]\n",
      "\n",
      "======================================================================\n",
      "✅ Tokenizer trained successfully!\n",
      "   Final vocab size: 40000\n",
      "   Saved to: medical_envi_tokenizer/\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers, processors\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import DatasetDict\n",
    "from tokenizers import decoders\n",
    "\n",
    "def train_medical_tokenizer(\n",
    "    dataset: DatasetDict,\n",
    "    vocab_size: int = 40000,\n",
    "    save_path: str = \"tokenizer_medical_envi\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a robust byte-level BPE tokenizer optimized for EN-VI medical translation.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"        TRAINING EN–VI MEDICAL BYTE-LEVEL TOKENIZER\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1. Build corpus generator\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\n1. Preparing training corpus...\")\n",
    "\n",
    "    train_iter = dataset[\"train\"].to_iterable_dataset()\n",
    "\n",
    "    def corpus_generator():\n",
    "        for ex in train_iter:\n",
    "            if \"en\" in ex and ex[\"en\"]:\n",
    "                yield ex[\"en\"]\n",
    "            if \"vi\" in ex and ex[\"vi\"]:\n",
    "                yield ex[\"vi\"]\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2. Create tokenizer\n",
    "    # --------------------------------------------------------\n",
    "    print(\"2. Initializing byte-level BPE tokenizer...\")\n",
    "\n",
    "    tokenizer = Tokenizer(\n",
    "    models.BPE(unk_token=\"<unk>\", byte_fallback=True)\n",
    "    )\n",
    "    # --- normalization pipeline ---\n",
    "    tokenizer.normalizer = normalizers.Sequence([\n",
    "        normalizers.NFD(),       # fully decompose (Vietnamese safe)\n",
    "        normalizers.Lowercase(), # lowercase everything\n",
    "        normalizers.NFC(),       # recompose\n",
    "        normalizers.Strip(),     # remove leading/trailing whitespace\n",
    "    ])\n",
    "\n",
    "    # --- byte-level pretokenizer ---\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "    # Add this line to your tokenizer setup\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3. Trainer configuration\n",
    "    # --------------------------------------------------------\n",
    "    print(\"3. Configuring BPE trainer...\")\n",
    "\n",
    "    special_tokens = [\n",
    "        \"<pad>\", \"<unk>\", \"<s>\", \"</s>\",\n",
    "        \"<en>\", \"<vi>\"   # language tags\n",
    "    ]\n",
    "\n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=2,\n",
    "        special_tokens=special_tokens,\n",
    "        initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
    "        show_progress=True,\n",
    "    )\n",
    "    # --------------------------------------------------------\n",
    "    # 4. Training\n",
    "    # --------------------------------------------------------\n",
    "    print(f\"4. Training tokenizer on corpus (vocab={vocab_size})...\")\n",
    "    tokenizer.train_from_iterator(corpus_generator(), trainer=trainer)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 5. Post-processing (add <s> and </s> automatically)\n",
    "    # --------------------------------------------------------\n",
    "    tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"<s> $A </s>\",\n",
    "        pair=\"<s> $A </s> <s> $B </s>\",\n",
    "        special_tokens=[\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 6. Wrap as HF tokenizer\n",
    "    # --------------------------------------------------------\n",
    "    wrapped = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        unk_token=\"<unk>\",\n",
    "        additional_special_tokens=[\"<en>\", \"<vi>\"],\n",
    "        model_max_length=512,\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 7. Save\n",
    "    # --------------------------------------------------------\n",
    "    print(f\"\\n5. Saving tokenizer to: {save_path}/\")\n",
    "    wrapped.save_pretrained(save_path)\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 8. Test samples\n",
    "    # --------------------------------------------------------\n",
    "    print(\"\\n6. Testing tokenizer with sample medical text:\")\n",
    "\n",
    "    tests = [\n",
    "        \"Với cả hai phương pháp, không có hai bản in nào giống bản nào, nhưng cả hai đều cho ra hình ảnh ấn tượng.\"\n",
    "    ]\n",
    "\n",
    "    for t in tests:\n",
    "        print(\"\\nText:\", t)\n",
    "        print(\"Tokens:\", wrapped.tokenize(t))\n",
    "        print(\"IDs:\", wrapped.encode(t))\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"✅ Tokenizer trained successfully!\")\n",
    "    print(f\"   Final vocab size: {wrapped.vocab_size}\")\n",
    "    print(f\"   Saved to: {save_path}/\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    return wrapped\n",
    "tokenizer = train_medical_tokenizer( dataset, vocab_size=40000, save_path=\"medical_envi_tokenizer\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dac7a430",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:46:23.783114Z",
     "iopub.status.busy": "2025-12-14T08:46:23.782686Z",
     "iopub.status.idle": "2025-12-14T08:46:23.789213Z",
     "shell.execute_reply": "2025-12-14T08:46:23.788505Z"
    },
    "papermill": {
     "duration": 0.014373,
     "end_time": "2025-12-14T08:46:23.790261",
     "exception": false,
     "start_time": "2025-12-14T08:46:23.775888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: \"nơi giáo dục và dìu dắt tôi để tôi có thể phụng sự nền Cộng Hòa\"\n",
      "Tokens: ['\"', 'nÆ¡i', 'ĠgiÃ¡o', 'Ġdá»¥c', 'ĠvÃł', 'ĠdÃ¬', 'u', 'Ġdáº¯t', 'ĠtÃ´i', 'ĠÄĳá»ĥ', 'ĠtÃ´i', 'ĠcÃ³', 'Ġthá»ĥ', 'Ġphá»¥ng', 'Ġsá»±', 'Ġná»ģn', 'Ġcá»Ļng', 'ĠhÃ²a', '\"']\n",
      "IDs: [7, 17277, 1249, 2514, 321, 8591, 90, 8809, 381, 527, 381, 362, 488, 14180, 555, 2480, 1519, 2058, 7]\n",
      "Text: \"education and leading me to providing service to the Republic.\"\n",
      "Tokens: ['\"', 'educ', 'ation', 'Ġand', 'Ġleading', 'Ġme', 'Ġto', 'Ġproviding', 'Ġservice', 'Ġto', 'Ġthe', 'Ġrepublic', '.\"']\n",
      "IDs: [7, 24241, 473, 332, 5302, 519, 315, 8660, 3251, 315, 281, 3541, 1097]\n"
     ]
    }
   ],
   "source": [
    "text = dataset[\"train\"][0][\"vi\"]\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"IDs:\", ids)\n",
    "\n",
    "\n",
    "text = dataset[\"train\"][0][\"en\"]\n",
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"IDs:\", ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0690268",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:46:23.799552Z",
     "iopub.status.busy": "2025-12-14T08:46:23.799011Z",
     "iopub.status.idle": "2025-12-14T08:46:23.803894Z",
     "shell.execute_reply": "2025-12-14T08:46:23.803223Z"
    },
    "papermill": {
     "duration": 0.010667,
     "end_time": "2025-12-14T08:46:23.805007",
     "exception": false,
     "start_time": "2025-12-14T08:46:23.794340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Chiếc cúp trong lòng toi là chiếc cúp quý giá nhất\n",
      "encoded: [2, 30438, 6753, 403, 2580, 26510, 330, 1513, 6753, 3020, 1166, 702, 3]\n",
      "Decoded:  <s>chiếc cúp trong lòng toi là chiếc cúp quý giá nhất</s>\n",
      "Match: False\n"
     ]
    }
   ],
   "source": [
    "# Encode then decode\n",
    "text = \"Chiếc cúp trong lòng toi là chiếc cúp quý giá nhất\"\n",
    "encoded = tokenizer.encode(text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"encoded: {encoded}\")\n",
    "print(f\"Decoded:  {decoded}\")\n",
    "print(f\"Match: {text == decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1deca9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:46:23.814018Z",
     "iopub.status.busy": "2025-12-14T08:46:23.813817Z",
     "iopub.status.idle": "2025-12-14T08:46:23.818943Z",
     "shell.execute_reply": "2025-12-14T08:46:23.818439Z"
    },
    "papermill": {
     "duration": 0.010763,
     "end_time": "2025-12-14T08:46:23.819951",
     "exception": false,
     "start_time": "2025-12-14T08:46:23.809188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch  # You also need this for torch.tensor inside __getitem__\n",
    "# ============================================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================================\n",
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"Dataset for EN-VI medical translation\"\"\"\n",
    "    def __init__(self, dataset, tokenizer, max_len=512):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.bos_id = tokenizer.bos_token_id\n",
    "        self.eos_id = tokenizer.eos_token_id\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        src_ids = self.tokenizer.encode(item['en'], add_special_tokens=True, max_length=self.max_len, truncation=True)\n",
    "        tgt_ids = self.tokenizer.encode(item['vi'], add_special_tokens=True, max_length=self.max_len, truncation=True)\n",
    "        \n",
    "        return {\n",
    "            'src': torch.tensor(src_ids, dtype=torch.long),\n",
    "            'tgt': torch.tensor(tgt_ids, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2b6c9b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:46:23.829366Z",
     "iopub.status.busy": "2025-12-14T08:46:23.828995Z",
     "iopub.status.idle": "2025-12-14T08:46:23.833315Z",
     "shell.execute_reply": "2025-12-14T08:46:23.832596Z"
    },
    "papermill": {
     "duration": 0.010194,
     "end_time": "2025-12-14T08:46:23.834434",
     "exception": false,
     "start_time": "2025-12-14T08:46:23.824240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_id=0):\n",
    "    \"\"\"Collate function with dynamic padding\"\"\"\n",
    "    src_batch = [item['src'] for item in batch]\n",
    "    tgt_batch = [item['tgt'] for item in batch]\n",
    "    \n",
    "    # Pad sequences\n",
    "    src_padded = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=pad_id)\n",
    "    tgt_padded = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=pad_id)\n",
    "    \n",
    "    return {\n",
    "        'src': src_padded,\n",
    "        'tgt': tgt_padded\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bba7189",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:46:23.843930Z",
     "iopub.status.busy": "2025-12-14T08:46:23.843436Z",
     "iopub.status.idle": "2025-12-14T08:46:23.849352Z",
     "shell.execute_reply": "2025-12-14T08:46:23.848807Z"
    },
    "papermill": {
     "duration": 0.011593,
     "end_time": "2025-12-14T08:46:23.850410",
     "exception": false,
     "start_time": "2025-12-14T08:46:23.838817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# ============================================================================\n",
    "# POSITIONAL ENCODING\n",
    "# ============================================================================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding for transformer\"\"\"\n",
    "    def __init__(self, d_model, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34286339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:46:23.859785Z",
     "iopub.status.busy": "2025-12-14T08:46:23.859398Z",
     "iopub.status.idle": "2025-12-14T08:46:23.867125Z",
     "shell.execute_reply": "2025-12-14T08:46:23.866415Z"
    },
    "papermill": {
     "duration": 0.013656,
     "end_time": "2025-12-14T08:46:23.868263",
     "exception": false,
     "start_time": "2025-12-14T08:46:23.854607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MULTI-HEAD ATTENTION\n",
    "# ============================================================================\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head self-attention mechanism\"\"\"\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split into multiple heads: (batch, seq_len, d_model) -> (batch, num_heads, seq_len, d_k)\"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Combine heads: (batch, num_heads, seq_len, d_k) -> (batch, seq_len, d_model)\"\"\"\n",
    "        batch_size, num_heads, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query, key, value: (batch_size, seq_len, d_model)\n",
    "            mask: (batch_size, 1, seq_len, seq_len) or (batch_size, 1, 1, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections and split heads\n",
    "        Q = self.split_heads(self.W_q(query))  # (batch, num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(self.W_k(key))\n",
    "        V = self.split_heads(self.W_v(value))\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_weights, V)  # (batch, num_heads, seq_len, d_k)\n",
    "        \n",
    "        # Combine heads and final linear\n",
    "        attn_output = self.combine_heads(attn_output)  # (batch, seq_len, d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fd02789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:46:23.877245Z",
     "iopub.status.busy": "2025-12-14T08:46:23.876978Z",
     "iopub.status.idle": "2025-12-14T08:46:23.881198Z",
     "shell.execute_reply": "2025-12-14T08:46:23.880507Z"
    },
    "papermill": {
     "duration": 0.009934,
     "end_time": "2025-12-14T08:46:23.882309",
     "exception": false,
     "start_time": "2025-12-14T08:46:23.872375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEED FORWARD NETWORK\n",
    "# ============================================================================\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network\"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c94bfc38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:46:23.891749Z",
     "iopub.status.busy": "2025-12-14T08:46:23.891240Z",
     "iopub.status.idle": "2025-12-14T08:46:23.896379Z",
     "shell.execute_reply": "2025-12-14T08:46:23.895825Z"
    },
    "papermill": {
     "duration": 0.011087,
     "end_time": "2025-12-14T08:46:23.897567",
     "exception": false,
     "start_time": "2025-12-14T08:46:23.886480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ENCODER LAYER\n",
    "# ============================================================================\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Single encoder layer with self-attention and feed-forward\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # Self-attention with residual connection\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9419162",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:46:23.907183Z",
     "iopub.status.busy": "2025-12-14T08:46:23.906783Z",
     "iopub.status.idle": "2025-12-14T08:46:23.912370Z",
     "shell.execute_reply": "2025-12-14T08:46:23.911658Z"
    },
    "papermill": {
     "duration": 0.011678,
     "end_time": "2025-12-14T08:46:23.913421",
     "exception": false,
     "start_time": "2025-12-14T08:46:23.901743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DECODER LAYER\n",
    "# ============================================================================\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Single decoder layer with self-attention, cross-attention, and feed-forward\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        # Self-attention on target\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Cross-attention on encoder output\n",
    "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout2(cross_attn_output))\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout3(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9487e5a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:46:23.922909Z",
     "iopub.status.busy": "2025-12-14T08:46:23.922503Z",
     "iopub.status.idle": "2025-12-14T08:46:23.932214Z",
     "shell.execute_reply": "2025-12-14T08:46:23.931678Z"
    },
    "papermill": {
     "duration": 0.015532,
     "end_time": "2025-12-14T08:46:23.933196",
     "exception": false,
     "start_time": "2025-12-14T08:46:23.917664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRANSFORMER MODEL\n",
    "# ============================================================================\n",
    "class TransformerTranslator(nn.Module):\n",
    "    \"\"\"Complete Transformer model for EN-VI medical translation\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        d_model=512,\n",
    "        num_heads=8,\n",
    "        num_encoder_layers=6,\n",
    "        num_decoder_layers=6,\n",
    "        d_ff=2048,\n",
    "        max_len=512,\n",
    "        dropout=0.1,\n",
    "        pad_idx=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        # Embeddings\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.decoder_embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        # Encoder and Decoder stacks\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier uniform\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"Create padding mask for source: (batch, 1, 1, src_len)\"\"\"\n",
    "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "    \n",
    "    def make_tgt_mask(self, tgt):\n",
    "        \"\"\"Create causal mask for target: (batch, 1, tgt_len, tgt_len)\"\"\"\n",
    "        batch_size, tgt_len = tgt.size()\n",
    "        \n",
    "        # Padding mask\n",
    "        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, tgt_len)\n",
    "        \n",
    "        # Causal mask (lower triangular)\n",
    "        tgt_sub_mask = torch.tril(torch.ones((tgt_len, tgt_len), device=tgt.device)).bool()\n",
    "        tgt_sub_mask = tgt_sub_mask.unsqueeze(0).unsqueeze(0)  # (1, 1, tgt_len, tgt_len)\n",
    "        \n",
    "        tgt_mask = tgt_pad_mask & tgt_sub_mask\n",
    "        return tgt_mask\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"Encode source sequence\"\"\"\n",
    "        # Embedding + positional encoding\n",
    "        x = self.encoder_embedding(src) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def decode(self, tgt, encoder_output, src_mask, tgt_mask):\n",
    "        \"\"\"Decode target sequence\"\"\"\n",
    "        # Embedding + positional encoding\n",
    "        x = self.decoder_embedding(tgt) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (batch_size, src_len)\n",
    "            tgt: (batch_size, tgt_len)\n",
    "        Returns:\n",
    "            output: (batch_size, tgt_len, vocab_size)\n",
    "        \"\"\"\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        tgt_mask = self.make_tgt_mask(tgt)\n",
    "        \n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        decoder_output = self.decode(tgt, encoder_output, src_mask, tgt_mask)\n",
    "        \n",
    "        output = self.output_projection(decoder_output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddd249f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:46:23.942763Z",
     "iopub.status.busy": "2025-12-14T08:46:23.942587Z",
     "iopub.status.idle": "2025-12-14T08:46:23.949399Z",
     "shell.execute_reply": "2025-12-14T08:46:23.948664Z"
    },
    "papermill": {
     "duration": 0.012619,
     "end_time": "2025-12-14T08:46:23.950405",
     "exception": false,
     "start_time": "2025-12-14T08:46:23.937786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, grad_clip=1.0):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        src = batch['src'].to(device)\n",
    "        tgt = batch['tgt'].to(device)\n",
    "        \n",
    "        # Teacher forcing: use tgt[:-1] as input, predict tgt[1:]\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src, tgt_input)  # (batch, tgt_len-1, vocab_size)\n",
    "        \n",
    "        # Compute loss\n",
    "        output = output.reshape(-1, output.size(-1))\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "        loss = criterion(output, tgt_output)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            src = batch['src'].to(device)\n",
    "            tgt = batch['tgt'].to(device)\n",
    "            \n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            output = model(src, tgt_input)\n",
    "            \n",
    "            output = output.reshape(-1, output.size(-1))\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            loss = criterion(output, tgt_output)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d1c593f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:46:23.959504Z",
     "iopub.status.busy": "2025-12-14T08:46:23.959328Z",
     "iopub.status.idle": "2025-12-14T08:46:28.412047Z",
     "shell.execute_reply": "2025-12-14T08:46:28.411334Z"
    },
    "papermill": {
     "duration": 4.458951,
     "end_time": "2025-12-14T08:46:28.413547",
     "exception": false,
     "start_time": "2025-12-14T08:46:23.954596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\r\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\r\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\r\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2025.11.3)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\r\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\r\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\r\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\r\n",
      "Installing collected packages: portalocker, sacrebleu\r\n",
      "Successfully installed portalocker-3.2.0 sacrebleu-2.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94cda0f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:46:28.424022Z",
     "iopub.status.busy": "2025-12-14T08:46:28.423805Z",
     "iopub.status.idle": "2025-12-14T08:46:28.429143Z",
     "shell.execute_reply": "2025-12-14T08:46:28.428565Z"
    },
    "papermill": {
     "duration": 0.011765,
     "end_time": "2025-12-14T08:46:28.430125",
     "exception": false,
     "start_time": "2025-12-14T08:46:28.418360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate_sentence(model, tokenizer, sentence, device, max_len=100):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input\n",
    "    encoded = tokenizer(\n",
    "        sentence,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=False,\n",
    "        truncation=True\n",
    "    )\n",
    "    src = encoded[\"input_ids\"].to(device)\n",
    "\n",
    "    # Decode using greedy search\n",
    "    pred_ids = greedy_decode(model, src, tokenizer, max_len=max_len)[0].tolist()\n",
    "\n",
    "    # Trim at EOS\n",
    "    if tokenizer.eos_token_id in pred_ids:\n",
    "        pred_ids = pred_ids[:pred_ids.index(tokenizer.eos_token_id)]\n",
    "\n",
    "    # Convert to text\n",
    "    translation = tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
    "    return translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b474c37b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:46:28.440444Z",
     "iopub.status.busy": "2025-12-14T08:46:28.439953Z",
     "iopub.status.idle": "2025-12-14T08:46:28.445430Z",
     "shell.execute_reply": "2025-12-14T08:46:28.444814Z"
    },
    "papermill": {
     "duration": 0.01187,
     "end_time": "2025-12-14T08:46:28.446481",
     "exception": false,
     "start_time": "2025-12-14T08:46:28.434611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['- No, I thought that was you.', 'In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.', \"So why should I trust you, Because you've already betrayed me.\", 'And we asked other people, how fast were the cars going when they smashed into each other?', 'Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_random_test_examples(dataset, n=5):\n",
    "    indices = random.sample(range(len(dataset['test'])), n)\n",
    "    return [dataset['test'][i]['en'] for i in indices]\n",
    "\n",
    "# Example: Sample 5 random English sentences\n",
    "medical_examples = get_random_test_examples(dataset, n=5)\n",
    "\n",
    "print(medical_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f63d3331",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T08:46:28.456776Z",
     "iopub.status.busy": "2025-12-14T08:46:28.456582Z",
     "iopub.status.idle": "2025-12-14T20:18:01.739439Z",
     "shell.execute_reply": "2025-12-14T20:18:01.738604Z"
    },
    "papermill": {
     "duration": 41519.364292,
     "end_time": "2025-12-14T20:18:27.815301",
     "exception": false,
     "start_time": "2025-12-14T08:46:28.451009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Model Parameters: 38,132,800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 1/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:03<00:00,  7.34it/s, loss=3.0453]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 22.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 4.2224\n",
      "Val Loss  : 2.6909\n",
      "\n",
      "Epoch 1/20\n",
      "Train Loss: 4.2224\n",
      "Val Loss  : 2.6909\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi nghĩ anh là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để đáp ứng, ba quyền lực tây ban nha phóng ra không khí để cung cấp công dân của berlin.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vậy tại sao tôi nên tin tưởng anh, bởi vì anh đã phản bội tôi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi những người khác, làm sao nhanh lên khi họ bị bắt vào mỗi người?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: loài chim và một số loài sát nhân có thể xác định, nhưng thay vì sự liên hệ của cha mẹ, sự liên hệ của họ đã được xác định bởi mẹ.\n",
      "---\n",
      "✓ Saved best model (val_loss=2.6909)\n",
      "\n",
      "============================================================\n",
      "Epoch 2/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:00<00:00,  7.35it/s, loss=2.7137]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.7582\n",
      "Val Loss  : 2.1844\n",
      "\n",
      "Epoch 2/20\n",
      "Train Loss: 2.7582\n",
      "Val Loss  : 2.1844\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi nghĩ đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để đáp ứng, ba cường quốc phương tây khởi động không quân berlin để cung cấp cho công dân berlin bằng không quân berlin.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vậy nên tại sao tôi tin anh, vì anh đã phản bội tôi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi người khác, nhanh như thế nào khi họ bị đập vào nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: loài chim và một số con bò sát có sự quyết định về di truyền của họ, nhưng thay vì giới tính được xác định bởi cha, tình dục của họ được xác định bởi mẹ.\n",
      "---\n",
      "✓ Saved best model (val_loss=2.1844)\n",
      "\n",
      "============================================================\n",
      "Epoch 3/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:02<00:00,  7.34it/s, loss=2.2418]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 22.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.4565\n",
      "Val Loss  : 2.0291\n",
      "\n",
      "Epoch 3/20\n",
      "Train Loss: 2.4565\n",
      "Val Loss  : 2.0291\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi nghĩ đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để đáp lại, ba cường quốc phương tây đã phóng lên không khí berlin để cung cấp công dân berlin bằng không khí.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vì vậy, tại sao tôi tin anh, vì anh đã phản bội tôi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi người khác, nhanh đến mức nào khi họ bị phá vỡ thành nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: loài chim và một số loài bò sát có sự quan hệ tình dục xác định, nhưng thay vì quan hệ tình dục được xác định bởi cha, tình dục của chúng được xác định bởi mẹ.\n",
      "---\n",
      "✓ Saved best model (val_loss=2.0291)\n",
      "\n",
      "============================================================\n",
      "Epoch 4/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:01<00:00,  7.34it/s, loss=2.2281]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 22.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.3300\n",
      "Val Loss  : 1.9582\n",
      "\n",
      "Epoch 4/20\n",
      "Train Loss: 2.3300\n",
      "Val Loss  : 1.9582\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi nghĩ đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: đáp lại, ba cường quốc phương tây ra mắt không khí berlin để cung cấp cho công dân berlin bằng không khí.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vậy tại sao tôi lại tin anh, bởi vì anh đã phản bội tôi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi những người khác, xe nhanh đến mức nào khi họ đập vào nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: các loài chim và một số loài bò sát có tính hệ sinh dục của chúng xác định, nhưng thay vì tình dục được xác định bởi cha, tình dục của chúng được xác định bởi mẹ.\n",
      "---\n",
      "✓ Saved best model (val_loss=1.9582)\n",
      "\n",
      "============================================================\n",
      "Epoch 5/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:13<00:00,  7.30it/s, loss=2.5383]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.2560\n",
      "Val Loss  : 1.9122\n",
      "\n",
      "Epoch 5/20\n",
      "Train Loss: 2.2560\n",
      "Val Loss  : 1.9122\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi nghĩ đó là cô.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để đáp ứng, ba cường quốc phương tây đã khởi động chuyến không quân berlin để cung cấp cho công dân berlin bằng không khí.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vậy tại sao tôi phải tin tưởng anh, bởi vì anh đã phản bội tôi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi những người khác, xe hơi nhanh thế nào khi họ đập vỡ nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: loài chim và một số bò sát có tính giới tính của họ xác định, nhưng thay vì giới tính được bố quyết định, giới tính của họ được xác định bởi mẹ.\n",
      "---\n",
      "✓ Saved best model (val_loss=1.9122)\n",
      "\n",
      "============================================================\n",
      "Epoch 6/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:18<00:00,  7.28it/s, loss=2.0654]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.2033\n",
      "Val Loss  : 1.8796\n",
      "\n",
      "Epoch 6/20\n",
      "Train Loss: 2.2033\n",
      "Val Loss  : 1.8796\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi nghĩ đó là cô.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để đáp ứng, ba cường quốc phía tây phóng lên không khí berlin để cung cấp cho công dân berlin bằng không khí.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vì thế tại sao tôi tin cô, bởi vì cô đã phản bội tôi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi những người khác, những chiếc xe này đi nhanh thế nào khi chúng bị đập vào nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: chim và một số bò sát có giới tính về mặt gen xác định, nhưng thay vì quan hệ tình dục được xác định bởi bố, tình dục của chúng được xác định bởi mẹ.\n",
      "---\n",
      "✓ Saved best model (val_loss=1.8796)\n",
      "\n",
      "============================================================\n",
      "Epoch 7/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:20<00:00,  7.28it/s, loss=2.0615]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.1617\n",
      "Val Loss  : 1.8522\n",
      "\n",
      "Epoch 7/20\n",
      "Train Loss: 2.1617\n",
      "Val Loss  : 1.8522\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi nghĩ đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để đáp ứng, ba cường quốc phương tây đã tung ra việc nâng không khí berlin cung cấp cho công dân berlin bằng không khí.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vậy sao tôi nên tin anh, bởi vì anh đã phản bội tôi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi những người khác, xe sẽ nhanh cỡ nào khi họ đập vào nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: chim và một số bò sát có tính giới tính quyết định về gen, nhưng thay vì giới tính được bố quyết tâm, tình dục của chúng được xác định bởi mẹ.\n",
      "---\n",
      "✓ Saved best model (val_loss=1.8522)\n",
      "\n",
      "============================================================\n",
      "Epoch 8/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:19<00:00,  7.28it/s, loss=2.4572]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.1285\n",
      "Val Loss  : 1.8356\n",
      "\n",
      "Epoch 8/20\n",
      "Train Loss: 2.1285\n",
      "Val Loss  : 1.8356\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi tưởng đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: đáp lại, ba cường quốc phương tây khởi động hàng không berlin để cung cấp cho công dân berlin bằng không khí.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vì thế tại sao tôi nên tin anh, vì anh đã phản bội tôi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi những người khác, xe nhanh đến mức nào khi họ đập vào nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: những con chim và một số loài bò sát có tính giới tính của chúng, nhưng thay vì giới tính được xác định bởi cha, tình dục của chúng được xác định bởi mẹ.\n",
      "---\n",
      "✓ Saved best model (val_loss=1.8356)\n",
      "\n",
      "============================================================\n",
      "Epoch 9/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:19<00:00,  7.28it/s, loss=2.1770]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.0998\n",
      "Val Loss  : 1.8201\n",
      "\n",
      "Epoch 9/20\n",
      "Train Loss: 2.0998\n",
      "Val Loss  : 1.8201\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi tưởng đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: đáp lại, ba cường quốc phương tây phóng chiếc máy bay áp berlin lên cung cấp cho công dân berlin bằng không khí.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vì thế tại sao tôi phải tin cô, vì cô đã phản bội tôi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi những người khác, những chiếc xe đó chạy nhanh thế nào khi họ đập lẫn nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: chim và một số loài bò sát có tính dục của chúng xác định về mặt di truyền, nhưng thay vì tình dục được bố xác định, tình dục của chúng được mẹ xác định.\n",
      "---\n",
      "✓ Saved best model (val_loss=1.8201)\n",
      "\n",
      "============================================================\n",
      "Epoch 10/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:18<00:00,  7.28it/s, loss=2.0748]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.0745\n",
      "Val Loss  : 1.8087\n",
      "\n",
      "Epoch 10/20\n",
      "Train Loss: 2.0745\n",
      "Val Loss  : 1.8087\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi nghĩ đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để đáp lại, ba cường quốc phương tây đã phóng không quân berlin lên để cung cấp cho công dân berlin bằng không khí.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vì vậy, tại sao tôi phải tin anh, bởi vì anh đã phản bội tôi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi người khác, xe tăng sẽ nhanh như thế nào khi họ đập nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: chim và một số bò sát có tính di truyền tình dục của chúng được xác định, nhưng thay vì tình dục được xác định bởi bố, tình dục của chúng được xác định bởi mẹ.\n",
      "---\n",
      "✓ Saved best model (val_loss=1.8087)\n",
      "\n",
      "============================================================\n",
      "Epoch 11/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:22<00:00,  7.27it/s, loss=2.2020]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.0507\n",
      "Val Loss  : 1.7876\n",
      "\n",
      "Epoch 11/20\n",
      "Train Loss: 2.0507\n",
      "Val Loss  : 1.7876\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi nghĩ đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để đáp ứng, ba cường quốc phương tây khởi động chuyến bay berlin để cung cấp cho công dân berlin bằng không khí.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vậy tại sao tôi phải tin tưởng anh, vì anh đã phản bội tôi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi người khác, xe điện sẽ nhanh như thế nào khi họ đập vào nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: chim và một số loài bò sát có tính chất về mặt di truyền, nhưng thay vì giới tính được xác định bởi cha, tình dục của chúng được xác định bởi mẹ.\n",
      "---\n",
      "✓ Saved best model (val_loss=1.7876)\n",
      "\n",
      "============================================================\n",
      "Epoch 12/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:19<00:00,  7.28it/s, loss=2.0929]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.0324\n",
      "Val Loss  : 1.7785\n",
      "\n",
      "Epoch 12/20\n",
      "Train Loss: 2.0324\n",
      "Val Loss  : 1.7785\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi nghĩ đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để phản ứng, ba cường quốc phương tây khởi động hàng không berlin để cung cấp cho công dân berlin bằng không khí.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vậy tại sao tôi nên tin anh, vì anh đã phản bội tôi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi những người khác, xe sẽ nhanh như thế nào khi họ đập vào nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: nhóm chim và một số loài bò sát đã xác định về mặt di truyền, nhưng thay vì giới tính được xác định bởi bố, tình dục của chúng được xác định bởi mẹ.\n",
      "---\n",
      "✓ Saved best model (val_loss=1.7785)\n",
      "\n",
      "============================================================\n",
      "Epoch 13/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:21<00:00,  7.27it/s, loss=1.9837]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.0223\n",
      "Val Loss  : 1.7770\n",
      "\n",
      "Epoch 13/20\n",
      "Train Loss: 2.0223\n",
      "Val Loss  : 1.7770\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi tưởng đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để đáp trả, ba cường quốc phương tây khởi động cuộc không kích berlin để cung cấp cho công dân berlin bằng cách không khí.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vậy sao tôi nên tin anh, vì anh đã phản bội tôi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi những người khác, xe chạy nhanh đến mức nào khi họ đổ nát nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: chim và một số loài bò sát có tính xác định về mặt di truyền, nhưng thay vì tình dục được xác định bởi cha, tình dục của chúng được mẹ xác định.\n",
      "---\n",
      "✓ Saved best model (val_loss=1.7770)\n",
      "\n",
      "============================================================\n",
      "Epoch 14/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:22<00:00,  7.27it/s, loss=2.0381]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 2.0085\n",
      "Val Loss  : 1.7677\n",
      "\n",
      "Epoch 14/20\n",
      "Train Loss: 2.0085\n",
      "Val Loss  : 1.7677\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi nghĩ đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để đáp lại, ba cường quốc phương tây phóng lên berlin air chưa được cung cấp cho công dân berlin.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vậy tại sao tôi lại tin tưởng anh, vì anh đã phản bội tôi rồi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi những người khác, những chiếc xe chạy nhanh đến mức nào khi họ đập nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: chim và một số loài bò sát đã xác định về mặt di truyền, nhưng thay vì tình dục được cha xác định, tình dục của chúng được mẹ xác định.\n",
      "---\n",
      "✓ Saved best model (val_loss=1.7677)\n",
      "\n",
      "============================================================\n",
      "Epoch 15/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:19<00:00,  7.28it/s, loss=1.9893]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.9948\n",
      "Val Loss  : 1.7556\n",
      "\n",
      "Epoch 15/20\n",
      "Train Loss: 1.9948\n",
      "Val Loss  : 1.7556\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi nghĩ đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để đáp lại, ba cường quốc phương tây đã phóng lên không khí berlin để cung cấp cho công dân berlin bằng không khí.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vì vậy tại sao tôi phải tin anh, vì anh đã phản bội tôi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi người khác, những chiếc xe sẽ nhanh đến mức nào khi họ đập vỡ nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: chim và một số loài bò sát đã xác định về mặt gen, nhưng thay vì giới tính được xác định bởi bố, giới tính của chúng được xác định bởi mẹ.\n",
      "---\n",
      "✓ Saved best model (val_loss=1.7556)\n",
      "\n",
      "============================================================\n",
      "Epoch 16/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:18<00:00,  7.28it/s, loss=1.8900]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.9778\n",
      "Val Loss  : 1.7529\n",
      "\n",
      "Epoch 16/20\n",
      "Train Loss: 1.9778\n",
      "Val Loss  : 1.7529\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi tưởng đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để đáp trả, ba cường quốc phương tây phóng không khí berlin lên để cung cấp cho công dân berlin bằng không khí.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vậy tại sao tôi phải tin anh, vì anh đã phản bội tôi rồi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi người khác, xe hơi sẽ nhanh như thế nào khi họ đập nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: chim và một số bò sát có tính chất tình dục của chúng xác định về mặt di truyền, nhưng thay vì tình dục được xác định bởi bố, tình dục của chúng được xác định bởi mẹ.\n",
      "---\n",
      "✓ Saved best model (val_loss=1.7529)\n",
      "\n",
      "============================================================\n",
      "Epoch 17/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:19<00:00,  7.28it/s, loss=2.1811]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.9586\n",
      "Val Loss  : 1.7302\n",
      "\n",
      "Epoch 17/20\n",
      "Train Loss: 1.9586\n",
      "Val Loss  : 1.7302\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi tưởng đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để đáp lại, ba cường quốc phương tây phóng sự gia tăng không quân berlin để cung cấp cho công dân berlin bằng không.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vậy tại sao tôi phải tin anh, bởi vì anh đã phản bội tôi rồi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi những người khác, xe hơi nhanh đến mức nào khi họ đập vào nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: đàn chim và một số bò sát đã xác định về mặt di truyền, nhưng thay vì tình dục được bố xác định, tình dục của chúng được xác định bởi mẹ.\n",
      "---\n",
      "✓ Saved best model (val_loss=1.7302)\n",
      "\n",
      "============================================================\n",
      "Epoch 18/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:19<00:00,  7.28it/s, loss=2.4310]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.9503\n",
      "Val Loss  : 1.7377\n",
      "\n",
      "Epoch 18/20\n",
      "Train Loss: 1.9503\n",
      "Val Loss  : 1.7377\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi tưởng đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để đáp lại, ba cường quốc phương tây phóng máy bay berlin để cung cấp cho công dân berlin bằng không khí.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vậy tại sao tôi nên tin anh, bởi vì anh đã phản bội tôi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi người khác, xe hơi nhanh đến mức nào khi họ đập vào nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: chim và một số bò sát có tình dục quyết định về mặt di truyền, nhưng thay vì tình dục được quyết định bởi bố, tình dục của chúng được xác định bởi mẹ.\n",
      "---\n",
      "\n",
      "============================================================\n",
      "Epoch 19/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:18<00:00,  7.28it/s, loss=2.1041]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.9417\n",
      "Val Loss  : 1.7264\n",
      "\n",
      "Epoch 19/20\n",
      "Train Loss: 1.9417\n",
      "Val Loss  : 1.7264\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi tưởng đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để đáp lại, ba cường quốc phương tây đã phóng lên không khí berlin để cung cấp cho công dân berlin bằng không khí.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vậy tại sao tôi nên tin anh, bởi vì anh đã phản bội tôi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi người khác, xe đi nhanh đến đâu khi họ đập nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: chim và một số bò sát có tính tình dục quyết tâm di truyền, nhưng thay vì giới tính được xác định bởi bố, tình dục của chúng được xác định bởi mẹ.\n",
      "---\n",
      "✓ Saved best model (val_loss=1.7264)\n",
      "\n",
      "============================================================\n",
      "Epoch 20/20\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/14992 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training: 100%|██████████| 14992/14992 [34:19<00:00,  7.28it/s, loss=1.9962]\n",
      "Evaluating:   0%|          | 0/172 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating: 100%|██████████| 172/172 [00:07<00:00, 21.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.9317\n",
      "Val Loss  : 1.7248\n",
      "\n",
      "Epoch 20/20\n",
      "Train Loss: 1.9317\n",
      "Val Loss  : 1.7248\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Sample Translations:\n",
      "EN: - No, I thought that was you.\n",
      "VI: - không, tôi nghĩ đó là anh.\n",
      "---\n",
      "EN: In response, the three Western powers launch the Berlin Airlift to supply the citizens of Berlin by air.\n",
      "VI: để đáp lại, ba cường quốc phương tây đã tung ra cuộc không kích berlin để cung cấp cho công dân berlin bằng không khí.\n",
      "---\n",
      "EN: So why should I trust you, Because you've already betrayed me.\n",
      "VI: vậy tại sao tôi nên tin anh, bởi vì anh đã phản bội tôi rồi.\n",
      "---\n",
      "EN: And we asked other people, how fast were the cars going when they smashed into each other?\n",
      "VI: và chúng tôi hỏi những người khác, những chiếc xe này đi nhanh đến mức nào khi bị phá vỡ vào nhau?\n",
      "---\n",
      "EN: Birds and some reptiles have their sex genetically determined, but instead of the sex being determined by dad, their sex is determined by mom.\n",
      "VI: chim và một số loài bò sát có tính tình dục được xác định di truyền, nhưng thay vì giới tính được xác định bởi bố, giới tính của chúng được xác định bởi mẹ.\n",
      "---\n",
      "✓ Saved best model (val_loss=1.7248)\n",
      "\n",
      "Training complete!\n",
      "\n",
      "Calculating BLEU on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU Score: 38.05\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FULL TRAINING SCRIPT (CLEAN + FIXED BLEU + FIXED DECODE)\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import sacrebleu\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Greedy Decode\n",
    "# ============================================================================\n",
    "def greedy_decode(model, src, tokenizer, max_len=100):\n",
    "    model.eval()\n",
    "    device = src.device\n",
    "\n",
    "    sos_id = tokenizer.bos_token_id\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Create source mask\n",
    "    src_mask = model.make_src_mask(src)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Encode source sequence\n",
    "        memory = model.encode(src, src_mask)\n",
    "\n",
    "        # Start decoder input with <sos>\n",
    "        ys = torch.full(\n",
    "            (src.size(0), 1),\n",
    "            fill_value=sos_id,\n",
    "            dtype=torch.long,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        for _ in range(max_len):\n",
    "\n",
    "            # Create target/causal mask\n",
    "            tgt_mask = model.make_tgt_mask(ys)\n",
    "\n",
    "            # Decode\n",
    "            out = model.decode(ys, memory, src_mask, tgt_mask)\n",
    "\n",
    "            # Project to vocab & pick top token\n",
    "            logits = model.output_projection(out[:, -1])  # last step\n",
    "            next_word = torch.argmax(logits, dim=-1).unsqueeze(1)\n",
    "\n",
    "            # Append\n",
    "            ys = torch.cat([ys, next_word], dim=1)\n",
    "\n",
    "            # Stop if all sentences predicted EOS\n",
    "            if (next_word == eos_id).all():\n",
    "                break\n",
    "\n",
    "    return ys\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Proper BLEU Computation (Correct sacrebleu Format)\n",
    "# ============================================================================\n",
    "\n",
    "def compute_bleu(model, dataloader, tokenizer, device):\n",
    "    model.eval()\n",
    "\n",
    "    hypotheses = []\n",
    "    reference_stream = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "\n",
    "        # match collate_fn keys\n",
    "        src = batch[\"src\"].to(device)\n",
    "        tgt = batch[\"tgt\"]\n",
    "\n",
    "        # Greedy decode predictions\n",
    "        pred_ids = greedy_decode(model, src, tokenizer, max_len=100)\n",
    "\n",
    "        for i in range(src.size(0)):\n",
    "\n",
    "            # ----- Decode Prediction -----\n",
    "            pred = pred_ids[i].tolist()\n",
    "            if tokenizer.eos_token_id in pred:\n",
    "                pred = pred[:pred.index(tokenizer.eos_token_id)]\n",
    "            pred_text = tokenizer.decode(pred, skip_special_tokens=True)\n",
    "\n",
    "            # ----- Decode Reference -----\n",
    "            ref = tgt[i].tolist()\n",
    "            if tokenizer.eos_token_id in ref:\n",
    "                ref = ref[:ref.index(tokenizer.eos_token_id)]\n",
    "            ref_text = tokenizer.decode(ref, skip_special_tokens=True)\n",
    "\n",
    "            hypotheses.append(pred_text)\n",
    "            reference_stream.append(ref_text)\n",
    "\n",
    "    bleu = sacrebleu.corpus_bleu(hypotheses, [reference_stream])\n",
    "    return bleu.score\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN TRAINING LOOP\n",
    "# ============================================================================\n",
    "\n",
    "def train():\n",
    "    import torch.optim as optim\n",
    "\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_EPOCHS = 20\n",
    "    LEARNING_RATE = 3e-4\n",
    "    D_MODEL = 256\n",
    "    NUM_HEADS = 8\n",
    "    NUM_LAYERS = 4\n",
    "    D_FF = 1024\n",
    "    DROPOUT = 0.1\n",
    "    MAX_LEN = 128\n",
    "    WARMUP_STEPS = 4000\n",
    "\n",
    "    # Device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(\"medical_envi_tokenizer\")\n",
    "\n",
    "    # Datasets\n",
    "    train_dataset = TranslationDataset(dataset[\"train\"], tokenizer, max_len=MAX_LEN)\n",
    "    val_dataset = TranslationDataset(dataset[\"validation\"], tokenizer, max_len=MAX_LEN)\n",
    "    test_dataset = TranslationDataset(dataset[\"test\"], tokenizer, max_len=MAX_LEN)\n",
    "    \n",
    "    # ✅ ADD THIS: Track metrics\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'learning_rate': []\n",
    "    }\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer.pad_token_id),\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer.pad_token_id),\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda b: collate_fn(b, tokenizer.pad_token_id),\n",
    "        num_workers=2\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = TransformerTranslator(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        num_encoder_layers=NUM_LAYERS,\n",
    "        num_decoder_layers=NUM_LAYERS,\n",
    "        d_ff=D_FF,\n",
    "        max_len=MAX_LEN,\n",
    "        dropout=DROPOUT,\n",
    "        pad_idx=tokenizer.pad_token_id\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"\\nModel Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    # Loss + Optimizer\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        betas=(0.9, 0.98),\n",
    "        eps=1e-9\n",
    "    )\n",
    "\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "    # Training\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss  : {val_loss:.4f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        # ✅ Store metrics\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss  : {val_loss:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        print(\"\\nSample Translations:\")\n",
    "        for s in medical_examples:\n",
    "            translation = translate_sentence(model, tokenizer, s, device)\n",
    "            print(f\"EN: {s}\")\n",
    "            print(f\"VI: {translation}\")\n",
    "            print(\"---\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"val_loss\": val_loss,\n",
    "            }, \"best_medical_translator.pt\")\n",
    "\n",
    "            print(f\"✓ Saved best model (val_loss={val_loss:.4f})\")\n",
    "\n",
    "    print(\"\\nTraining complete!\")\n",
    "\n",
    "    # ============================================================================\n",
    "    # BLEU Evaluation\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"\\nCalculating BLEU on test set...\")\n",
    "    bleu_score = compute_bleu(model, test_loader, tokenizer, device)\n",
    "    print(f\"\\nBLEU Score: {bleu_score:.2f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "\n",
    "# Run training\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d925639a",
   "metadata": {
    "papermill": {
     "duration": 25.470127,
     "end_time": "2025-12-14T20:19:19.291351",
     "exception": false,
     "start_time": "2025-12-14T20:18:53.821224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 285362068,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 286061863,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 41743.610837,
   "end_time": "2025-12-14T20:19:47.759479",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-14T08:44:04.148642",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "086e28436f934e369bd9d97c501d8f99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ed5cc675dc7a4ad1be5d4782ed5992b4",
       "placeholder": "​",
       "style": "IPY_MODEL_8a625d725b7e49369705a573aa6a4c40",
       "tabbable": null,
       "tooltip": null,
       "value": "Generating test split: "
      }
     },
     "0f8b200b515b4cdb9605d3555a15d562": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f648e1565cba468589e309aaaed304e4",
        "IPY_MODEL_4580323a63ef465ebbb54c244f10b727",
        "IPY_MODEL_8f081e28f5d34a92a1e2702e53369e3c"
       ],
       "layout": "IPY_MODEL_caed5dd9feb5452a837718e3a386665b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "159b645f3774458c963372b19f5c543e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1f09e9b2731a46368682705c3dc9495f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "20b8c165b3fb48cdbfc888af22a3df89": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2901d58d930d4a87b3f9c5dbb5b3f3ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2930d62dacb84310b8e633384d47b850": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_44246ab409d24d68bd4db71da70b1032",
        "IPY_MODEL_9dd6bfe62b1c477392452666f833643b",
        "IPY_MODEL_4d0514a949d34a9a9d7f25d849462742"
       ],
       "layout": "IPY_MODEL_ef4a23c9fa654df4b4d848dbf637dba1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "2b6661387f4e4421899bcde3e310e80e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2fcd722a26434c37bdddf391befda105": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_f472ce63783e4b8596c1948eb641cb2d",
       "placeholder": "​",
       "style": "IPY_MODEL_2b6661387f4e4421899bcde3e310e80e",
       "tabbable": null,
       "tooltip": null,
       "value": " 10917/0 [00:00&lt;00:00, 427636.86 examples/s]"
      }
     },
     "35a0470919624e67b86ead31e7ac00fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1f09e9b2731a46368682705c3dc9495f",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_847de63dd8834943afce61b7e3bed18c",
       "tabbable": null,
       "tooltip": null,
       "value": 3.0
      }
     },
     "3f811be639eb47778212a54ce4ca728f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4fb461f1567e48aa83da6fb7901851b6",
       "placeholder": "​",
       "style": "IPY_MODEL_159b645f3774458c963372b19f5c543e",
       "tabbable": null,
       "tooltip": null,
       "value": "Computing checksums: 100%"
      }
     },
     "44246ab409d24d68bd4db71da70b1032": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_20b8c165b3fb48cdbfc888af22a3df89",
       "placeholder": "​",
       "style": "IPY_MODEL_ba2474f520d242a28f573bfd9fd73d22",
       "tabbable": null,
       "tooltip": null,
       "value": "Generating val split: "
      }
     },
     "444110b65da74cc5b7b2c2a6685cf655": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "4580323a63ef465ebbb54c244f10b727": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_47f3017e21c645ca863a59617c0494a3",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2901d58d930d4a87b3f9c5dbb5b3f3ff",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "47f3017e21c645ca863a59617c0494a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "4c9a418047cc4cc89ef73829cc06114f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4d0514a949d34a9a9d7f25d849462742": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_df2bfa9ee5b040d8af586aa95d1810d7",
       "placeholder": "​",
       "style": "IPY_MODEL_c2b59dbe760346e3a33ea1f33bfd63fc",
       "tabbable": null,
       "tooltip": null,
       "value": " 11004/0 [00:00&lt;00:00, 406490.24 examples/s]"
      }
     },
     "4e4a7aed99c841ca80ac2ee47966b5c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4efda127781e4c37b52001e4a16c6488": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3f811be639eb47778212a54ce4ca728f",
        "IPY_MODEL_35a0470919624e67b86ead31e7ac00fc",
        "IPY_MODEL_ec3d41e7773b45ef8d19dd50eba247ff"
       ],
       "layout": "IPY_MODEL_82a545fd89e64c30911e2d4fbdda0463",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4fb461f1567e48aa83da6fb7901851b6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5a15cbcf510a467b8f4e8d8cdb559163": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6ad2ec483a4d4e36bb2a24ea75f59b41": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_086e28436f934e369bd9d97c501d8f99",
        "IPY_MODEL_bfc4f5e6f3054f4e8b1fdc8943892e65",
        "IPY_MODEL_2fcd722a26434c37bdddf391befda105"
       ],
       "layout": "IPY_MODEL_8122ed59423b4fadacbc11120f61ea1b",
       "tabbable": null,
       "tooltip": null
      }
     },
     "799593f9121a449f9a18eed3202f19f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8122ed59423b4fadacbc11120f61ea1b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "82a545fd89e64c30911e2d4fbdda0463": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "847de63dd8834943afce61b7e3bed18c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8a625d725b7e49369705a573aa6a4c40": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8f081e28f5d34a92a1e2702e53369e3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bd4779991fcd4e19b467c7683e533554",
       "placeholder": "​",
       "style": "IPY_MODEL_a571c15eaf57495c881cdb97fd4e97f6",
       "tabbable": null,
       "tooltip": null,
       "value": " 959482/0 [00:01&lt;00:00, 598181.51 examples/s]"
      }
     },
     "958eb36d38e64df99cf74c176e88b06c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "9dd6bfe62b1c477392452666f833643b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_958eb36d38e64df99cf74c176e88b06c",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_799593f9121a449f9a18eed3202f19f6",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "a571c15eaf57495c881cdb97fd4e97f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ba2474f520d242a28f573bfd9fd73d22": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bd4779991fcd4e19b467c7683e533554": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bfc4f5e6f3054f4e8b1fdc8943892e65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_444110b65da74cc5b7b2c2a6685cf655",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4c9a418047cc4cc89ef73829cc06114f",
       "tabbable": null,
       "tooltip": null,
       "value": 1.0
      }
     },
     "c2b59dbe760346e3a33ea1f33bfd63fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "caed5dd9feb5452a837718e3a386665b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "de7e071ecc0948f78d8c64501704a405": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "df2bfa9ee5b040d8af586aa95d1810d7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ec3d41e7773b45ef8d19dd50eba247ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_de7e071ecc0948f78d8c64501704a405",
       "placeholder": "​",
       "style": "IPY_MODEL_4e4a7aed99c841ca80ac2ee47966b5c4",
       "tabbable": null,
       "tooltip": null,
       "value": " 3/3 [00:00&lt;00:00, 557.06it/s]"
      }
     },
     "ed5cc675dc7a4ad1be5d4782ed5992b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ef4a23c9fa654df4b4d848dbf637dba1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f472ce63783e4b8596c1948eb641cb2d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f584085c7e594859b86ac37d7b9dbb9c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f648e1565cba468589e309aaaed304e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5a15cbcf510a467b8f4e8d8cdb559163",
       "placeholder": "​",
       "style": "IPY_MODEL_f584085c7e594859b86ac37d7b9dbb9c",
       "tabbable": null,
       "tooltip": null,
       "value": "Generating train split: "
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
