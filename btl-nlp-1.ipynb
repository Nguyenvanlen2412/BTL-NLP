{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load your dataset (replace with the actual name you are using)\ndataset = load_dataset(\"ncduy/mt-en-vi\")\n\n# 1. Inspect the structure\nprint(\"Sample:\", dataset[\"train\"][0]) \n# Expected Output: {'translation': {'en': '...', 'vi': '...'}}\n# If your columns are different (e.g., 'source', 'target'), you'll need to rename them.\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:28:58.614331Z","iopub.execute_input":"2025-12-02T16:28:58.614587Z","iopub.status.idle":"2025-12-02T16:29:29.079800Z","shell.execute_reply.started":"2025-12-02T16:28:58.614559Z","shell.execute_reply":"2025-12-02T16:29:29.078928Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff6719ccb0d4ff79265e31e99f380d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/597M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95c0cfb7bd1c433bbbcef58facf78d4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"valid.csv:   0%|          | 0.00/2.45M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86b2389b9c724d54b58a7b6d8be90a86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/2.43M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c46c857c02e7403789e1412a7ea10fd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2884451 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4c42050d213430285e3c2031aecad07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11316 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"454ef7e3dbc04f6aac1bf82fb14a2d22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/11225 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"186110e5f5bc44508b03a359407c36cd"}},"metadata":{}},{"name":"stdout","text":"Sample: {'en': \"- Sorry, that question's not on here.\", 'vi': '- Xin lỗi, nhưng mà ở đây không có câu hỏi đấy.', 'source': 'OpenSubtitles v2018'}\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# 2. Basic Filtering (Optional but recommended)\n# Remove sentences that are too long or too short to save memory\ndef is_valid_length(example):\n    en_len = len(example['en'].split())\n    vi_len = len(example['vi'].split())\n    return 1 < en_len < 50 and 1 < vi_len < 50\n\ncleaned_dataset = {\n    split: ds.filter(is_valid_length)\n    for split, ds in dataset.items()\n}\ncleaned_dataset[\"train\"] = cleaned_dataset[\"train\"].shuffle(seed=42).select(range(400000))\nprint(f\"Original: {len(dataset)}, Cleaned: {len(cleaned_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:29:29.081686Z","iopub.execute_input":"2025-12-02T16:29:29.082167Z","iopub.status.idle":"2025-12-02T16:29:54.814454Z","shell.execute_reply.started":"2025-12-02T16:29:29.082145Z","shell.execute_reply":"2025-12-02T16:29:54.813403Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/2884451 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1de84ceccc914921b7b10e5c45eab00c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/11316 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e33dcf5ecbc040988b1f490082033f9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/11225 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a24a7c0306e4e719a3e2b677349e651"}},"metadata":{}},{"name":"stdout","text":"Original: 3, Cleaned: 3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 1. Define the function to clean the text\ndef clean_and_format(example):\n    # This modifies the dictionary in-place\n    example['en'] = example['en'].lstrip('- ').strip()\n    example['vi'] = example['vi'].lstrip('- ').strip()\n    return example\n\n# 2. Apply it using .map()\n# We use 'remove_columns' to delete the 'source' column at the same time\ncleaned_dataset = {\n    split : ds.map(clean_and_format, remove_columns=['source'])\n    for split, ds in cleaned_dataset.items()\n}\n\n# Check the result\nprint(cleaned_dataset[\"train\"][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:29:54.815493Z","iopub.execute_input":"2025-12-02T16:29:54.815863Z","iopub.status.idle":"2025-12-02T16:30:40.074713Z","shell.execute_reply.started":"2025-12-02T16:29:54.815840Z","shell.execute_reply":"2025-12-02T16:30:40.073851Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/400000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"249bfd9fbcc74fa68c474d942b15f553"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10940 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a751d39c8f284803b7387d9eea5a5d87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10834 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2dd8010217248b18a643b12a170a8b2"}},"metadata":{}},{"name":"stdout","text":"{'en': \"I'm not sure you're that much of a patriot.\", 'vi': 'Tôi không nghĩ anh là 1 người yêu nước.'}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers, decoders\n\nprint(\"Starting tokenizer training...\")\ntokenizer = Tokenizer(models.BPE(unk_token=\"<unk>\"))\nprint(\"✓ Tokenizer initialized\")\n\n# Use NFC for Vietnamese - preserves diacritics correctly\ntokenizer.normalizer = normalizers.NFC()\nprint(\"✓ Normalizer configured (NFC)\")\n\n# Whitespace + punctuation split (subword-friendly)\ntokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\ntokenizer.decoder = decoders.BPEDecoder()\nprint(\"✓ Using Whitespace pre-tokenizer & BPE decoder\")\n\ntrainer = trainers.BpeTrainer(\n    vocab_size=32000,\n    special_tokens=[\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n)\nprint(\"✓ Trainer configured with vocab_size=32000\")\n\ndef text_iterator():\n    for ex in cleaned_dataset[\"train\"]:\n        yield ex[\"en\"]\n        yield ex[\"vi\"]\n\nprint(\"\\nTraining tokenizer...\")\ntokenizer.train_from_iterator(text_iterator(), trainer)\nprint(\"✓ Training completed\")\n\ntokenizer.save(\"bilingual_tokenizer.json\")\nprint(\"✓ Tokenizer saved to 'bilingual_tokenizer.json'\")\n\nprint(\"\\nExtracting special token IDs...\")\nspecial_tokens = {\n    \"pad\": tokenizer.token_to_id(\"<pad>\"),\n    \"unk\": tokenizer.token_to_id(\"<unk>\"),\n    \"sos\": tokenizer.token_to_id(\"<sos>\"),\n    \"eos\": tokenizer.token_to_id(\"<eos>\")\n}\nprint(special_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:30:40.075782Z","iopub.execute_input":"2025-12-02T16:30:40.076085Z","iopub.status.idle":"2025-12-02T16:31:06.215502Z","shell.execute_reply.started":"2025-12-02T16:30:40.076040Z","shell.execute_reply":"2025-12-02T16:31:06.214640Z"}},"outputs":[{"name":"stdout","text":"Starting tokenizer training...\n✓ Tokenizer initialized\n✓ Normalizer configured (NFC)\n✓ Using Whitespace pre-tokenizer & BPE decoder\n✓ Trainer configured with vocab_size=32000\n\nTraining tokenizer...\n\n\n\n✓ Training completed\n✓ Tokenizer saved to 'bilingual_tokenizer.json'\n\nExtracting special token IDs...\n{'pad': 0, 'unk': 1, 'sos': 2, 'eos': 3}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#Encode dataset with tokenizer\nfrom tokenizers import Tokenizer\nimport torch\n\ntokenizer = Tokenizer.from_file(\"bilingual_tokenizer.json\")\n\npad_id = tokenizer.token_to_id(\"<pad>\")\nsos_id = tokenizer.token_to_id(\"<sos>\")\neos_id = tokenizer.token_to_id(\"<eos>\")\n\ndef encode(text):\n    ids = tokenizer.encode(text).ids\n    return ids\n\n\nencoded_dataset_train = []\n\nfor ex in cleaned_dataset[\"train\"]:\n    src = encode(ex[\"en\"])\n    tgt = encode(ex[\"vi\"])\n    encoded_dataset_train.append((src, tgt))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T17:02:57.931093Z","iopub.execute_input":"2025-12-02T17:02:57.931396Z","iopub.status.idle":"2025-12-02T17:04:04.450393Z","shell.execute_reply.started":"2025-12-02T17:02:57.931375Z","shell.execute_reply":"2025-12-02T17:04:04.449206Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/2143176558.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mencoded_dataset_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcleaned_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"valid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vi\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'valid'"],"ename":"KeyError","evalue":"'valid'","output_type":"error"}],"execution_count":32},{"cell_type":"code","source":"encoded_dataset_valid = []\n\nfor ex in cleaned_dataset[\"validation\"]:\n    src = encode(ex[\"en\"])\n    tgt = encode(ex[\"vi\"])\n    encoded_dataset_valid.append((src, tgt))\n\nencoded_dataset_test = []\n\nfor ex in cleaned_dataset[\"test\"]:\n    src = encode(ex[\"en\"])\n    tgt = encode(ex[\"vi\"])\n    encoded_dataset_test.append((src, tgt))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T17:05:12.483561Z","iopub.execute_input":"2025-12-02T17:05:12.483866Z","iopub.status.idle":"2025-12-02T17:05:16.233824Z","shell.execute_reply.started":"2025-12-02T17:05:12.483843Z","shell.execute_reply":"2025-12-02T17:05:16.232940Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"#Build PyTorch Dataset\nclass TranslationDataset(torch.utils.data.Dataset):\n    def __init__(self, pairs, sos_id, eos_id):\n        self.pairs = pairs\n        self.sos_id = sos_id\n        self.eos_id = eos_id\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        src, tgt = self.pairs[idx]\n\n        # Add <sos> and <eos>\n        src = src + [self.eos_id]\n        tgt_input = [self.sos_id] + tgt\n        tgt_output = tgt + [self.eos_id]\n\n        return torch.tensor(src, dtype=torch.long), \\\n               torch.tensor(tgt_input, dtype=torch.long), \\\n               torch.tensor(tgt_output, dtype=torch.long)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T17:05:22.259408Z","iopub.execute_input":"2025-12-02T17:05:22.259725Z","iopub.status.idle":"2025-12-02T17:05:22.266294Z","shell.execute_reply.started":"2025-12-02T17:05:22.259702Z","shell.execute_reply":"2025-12-02T17:05:22.265296Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"#Build collate function (batching + padding)\n\ndef collate_fn(batch):\n    src_batch, tgt_in_batch, tgt_out_batch = zip(*batch)\n\n    #pad to the same length\n    src_padded = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first = True, padding_value = pad_id)\n    tgt_in_padded = torch.nn.utils.rnn.pad_sequence(tgt_in_batch, batch_first = True, padding_value = pad_id)\n    tgt_out_padded = torch.nn.utils.rnn.pad_sequence(tgt_out_batch, batch_first = True, padding_value = pad_id)\n\n    return src_padded, tgt_in_padded, tgt_out_padded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T17:05:34.964229Z","iopub.execute_input":"2025-12-02T17:05:34.964999Z","iopub.status.idle":"2025-12-02T17:05:34.971627Z","shell.execute_reply.started":"2025-12-02T17:05:34.964964Z","shell.execute_reply":"2025-12-02T17:05:34.970616Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"print(cleaned_dataset[\"validation\"][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T17:05:37.751576Z","iopub.execute_input":"2025-12-02T17:05:37.751877Z","iopub.status.idle":"2025-12-02T17:05:37.757347Z","shell.execute_reply.started":"2025-12-02T17:05:37.751855Z","shell.execute_reply":"2025-12-02T17:05:37.756337Z"}},"outputs":[{"name":"stdout","text":"{'en': 'In August 1764, Bertin permitted the export of grain from twenty-seven French ports, later expanded to thirty-six.', 'vi': 'Tháng 8 năm 1764, Bertin lại cho phép xuất khẩu ngũ cốc từ 27 cảng của Pháp, sau đó mở rộng lên 36.'}\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"#Create dataloader\ntrain_dataset = TranslationDataset(encoded_dataset_train, sos_id, eos_id)\nvalid_dataset = TranslationDataset(encoded_dataset_valid, sos_id, eos_id)\ntest_dataset = TranslationDataset(encoded_dataset_test, sos_id, eos_id)\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=64,\n    shuffle=True,\n    collate_fn=collate_fn\n)\n\nvalid_loader = torch.utils.data.DataLoader(\n    valid_dataset,\n    batch_size=64,\n    shuffle=False,\n    collate_fn=collate_fn\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=64,\n    shuffle=False,\n    collate_fn=collate_fn\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T17:05:39.408260Z","iopub.execute_input":"2025-12-02T17:05:39.408572Z","iopub.status.idle":"2025-12-02T17:05:39.414890Z","shell.execute_reply.started":"2025-12-02T17:05:39.408548Z","shell.execute_reply":"2025-12-02T17:05:39.413997Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\n\nclass PositionalEncoding(nn.Module):\n    \"\"\"Sinusoidal positional encoding\"\"\"\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        # Create positional encoding matrix\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len).unsqueeze(1).float()\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                            -(math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        # x: (batch_size, seq_len, d_model)\n        return x + self.pe[:, :x.size(1)]\n\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multi-head self-attention mechanism\"\"\"\n    def __init__(self, d_model, num_heads, dropout=0.1):\n        super().__init__()\n        assert d_model % num_heads == 0\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # Q, K, V: (batch_size, num_heads, seq_len, d_k)\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        attn_weights = torch.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n        \n        output = torch.matmul(attn_weights, V)\n        return output\n    \n    def forward(self, query, key, value, mask=None):\n        batch_size = query.size(0)\n        \n        # Linear projections\n        Q = self.W_q(query)  # (batch_size, seq_len, d_model)\n        K = self.W_k(key)\n        V = self.W_v(value)\n        \n        # Split into multiple heads\n        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n        \n        # Apply attention\n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        \n        # Concatenate heads\n        attn_output = attn_output.transpose(1, 2).contiguous()\n        attn_output = attn_output.view(batch_size, -1, self.d_model)\n        \n        # Final linear projection\n        output = self.W_o(attn_output)\n        return output\n\n\nclass FeedForward(nn.Module):\n    \"\"\"Position-wise feed-forward network\"\"\"\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.relu = nn.ReLU()\n    \n    def forward(self, x):\n        return self.linear2(self.dropout(self.relu(self.linear1(x))))\n\n\nclass EncoderLayer(nn.Module):\n    \"\"\"Single encoder layer\"\"\"\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # Self-attention with residual connection\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Feed-forward with residual connection\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        \n        return x\n\n\nclass DecoderLayer(nn.Module):\n    \"\"\"Single decoder layer\"\"\"\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n        # Masked self-attention with residual connection\n        attn_output = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        # Cross-attention with residual connection\n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        \n        # Feed-forward with residual connection\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n        \n        return x\n\n\nclass Transformer(nn.Module):\n    \"\"\"Complete Transformer model for translation\"\"\"\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, \n                 num_heads=8, num_encoder_layers=6, num_decoder_layers=6,\n                 d_ff=2048, dropout=0.1, max_len=5000, pad_idx=0):\n        super().__init__()\n        \n        self.d_model = d_model\n        self.pad_idx = pad_idx\n        \n        # Embeddings\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model, padding_idx=pad_idx)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model, padding_idx=pad_idx)\n        self.pos_encoding = PositionalEncoding(d_model, max_len)\n        \n        # Encoder and Decoder stacks\n        self.encoder_layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_encoder_layers)\n        ])\n        \n        self.decoder_layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff, dropout)\n            for _ in range(num_decoder_layers)\n        ])\n        \n        # Output projection\n        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        \n        # Initialize parameters\n        self._init_parameters()\n    \n    def _init_parameters(self):\n        \"\"\"Initialize parameters with Xavier uniform\"\"\"\n        for p in self.parameters():\n            if p.dim() > 1:\n                nn.init.xavier_uniform_(p)\n    \n    def create_src_mask(self, src):\n        \"\"\"Create mask for source padding\"\"\"\n        # src: (batch_size, src_len)\n        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\n        # (batch_size, 1, 1, src_len)\n        return src_mask\n    \n    def create_tgt_mask(self, tgt):\n        \"\"\"Create mask for target (padding + look-ahead)\"\"\"\n        batch_size, tgt_len = tgt.size()\n        \n        # Padding mask\n        tgt_pad_mask = (tgt != self.pad_idx).unsqueeze(1).unsqueeze(2)\n        # (batch_size, 1, 1, tgt_len)\n        \n        # Look-ahead mask (lower triangular)\n        tgt_sub_mask = torch.tril(torch.ones(tgt_len, tgt_len, device=tgt.device)).bool()\n        # (tgt_len, tgt_len)\n        \n        tgt_mask = tgt_pad_mask & tgt_sub_mask\n        # (batch_size, 1, tgt_len, tgt_len)\n        return tgt_mask\n    \n    def encode(self, src, src_mask):\n        \"\"\"Encode source sequence\"\"\"\n        # Embedding + positional encoding\n        x = self.src_embedding(src) * math.sqrt(self.d_model)\n        x = self.pos_encoding(x)\n        x = self.dropout(x)\n        \n        # Pass through encoder layers\n        for layer in self.encoder_layers:\n            x = layer(x, src_mask)\n        \n        return x\n    \n    def decode(self, tgt, enc_output, src_mask, tgt_mask):\n        \"\"\"Decode target sequence\"\"\"\n        # Embedding + positional encoding\n        x = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n        x = self.pos_encoding(x)\n        x = self.dropout(x)\n        \n        # Pass through decoder layers\n        for layer in self.decoder_layers:\n            x = layer(x, enc_output, src_mask, tgt_mask)\n        \n        return x\n    \n    def forward(self, src, tgt):\n        \"\"\"Forward pass\n        Args:\n            src: (batch_size, src_len)\n            tgt: (batch_size, tgt_len)\n        Returns:\n            output: (batch_size, tgt_len, tgt_vocab_size)\n        \"\"\"\n        src_mask = self.create_src_mask(src)\n        tgt_mask = self.create_tgt_mask(tgt)\n        \n        enc_output = self.encode(src, src_mask)\n        dec_output = self.decode(tgt, enc_output, src_mask, tgt_mask)\n        \n        output = self.fc_out(dec_output)\n        return output\n\n\n# Assuming you have tokenizer loaded\nvocab_size = tokenizer.get_vocab_size()\n\n# Initialize model\nmodel = Transformer(\n    src_vocab_size=vocab_size,\n    tgt_vocab_size=vocab_size,\n    d_model=512,\n    num_heads=8,\n    num_encoder_layers=6,\n    num_decoder_layers=6,\n    d_ff=2048,\n    dropout=0.1,\n    pad_idx=pad_id\n)\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\nprint(sum(p.numel() for p in model.parameters()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T17:05:41.984676Z","iopub.execute_input":"2025-12-02T17:05:41.984964Z","iopub.status.idle":"2025-12-02T17:05:43.426467Z","shell.execute_reply.started":"2025-12-02T17:05:41.984946Z","shell.execute_reply":"2025-12-02T17:05:43.425522Z"}},"outputs":[{"name":"stdout","text":"93322496\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport sys\nimport time\n\ndef print_progress(current, total, prefix=\"\"):\n    percent = 100 * (current / total)\n    bar_len = 30\n    filled = int(bar_len * current // total)\n    bar = \"#\" * filled + \"-\" * (bar_len - filled)\n    sys.stdout.write(f\"\\r{prefix} [{bar}] {percent:5.1f}%\")\n    sys.stdout.flush()\n\ndef train_epoch(model, train_loader, optimizer, criterion, device, epoch_idx, num_epochs):\n    model.train()\n    total_loss = 0\n    total_batches = len(train_loader)\n\n    for i, (src, tgt_in, tgt_out) in enumerate(train_loader, 1):        \n        src = src.to(device)\n        tgt_input = tgt_in.to(device)\n        tgt_output = tgt_out.to(device)\n\n        optimizer.zero_grad()\n\n        # forward\n        output = model(src, tgt_input)\n\n        # loss\n        output = output.reshape(-1, output.size(-1))\n        tgt_output = tgt_output.reshape(-1)\n        loss = criterion(output, tgt_output)\n\n        # backward\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        print_progress(i, total_batches, prefix=f\"Epoch {epoch_idx}/{num_epochs}\")\n\n    print()\n    return total_loss / total_batches\n\n\ndef evaluate(model, valid_loader, criterion, device):\n    model.eval()\n    total_loss = 0\n\n    with torch.no_grad():\n        for src, tgt_in, tgt_out in valid_loader:\n            src = src.to(device)\n            tgt_input = tgt_in.to(device)\n            tgt_output = tgt_out.to(device)\n\n            output = model(src, tgt_input)\n\n            output = output.reshape(-1, output.size(-1))\n            tgt_output = tgt_output.reshape(-1)\n            loss = criterion(output, tgt_output)\n\n            total_loss += loss.item()\n\n    return total_loss / len(valid_loader)\n\n\n# Training setup\ncriterion = nn.CrossEntropyLoss(ignore_index=special_tokens['pad'])\noptimizer = Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n\nnum_epochs = 15\nbest_val_loss = float('inf')\n\nprint(\"\\nStarting training...\\n\")\nfor epoch in range(1, num_epochs + 1):\n    start_time = time.time()\n    \n    train_loss = train_epoch(model, train_loader, optimizer, criterion, device, epoch, num_epochs)\n    val_loss = evaluate(model, valid_loader, criterion, device)\n    \n    scheduler.step(val_loss)\n    epoch_time = time.time() - start_time\n    \n    print(f\"Epoch {epoch}/{num_epochs} | \"\n          f\"Train Loss: {train_loss:.4f} | \"\n          f\"Val Loss: {val_loss:.4f} | \"\n          f\"Time: {epoch_time:.2f}s\")\n    \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': val_loss,\n        }, 'best_model.pt')\n        print(f\"✓ Saved best model (val_loss: {val_loss:.4f})\\n\")\n\nprint(\"\\nTraining completed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T17:09:55.609862Z","iopub.execute_input":"2025-12-02T17:09:55.610263Z","iopub.status.idle":"2025-12-02T17:10:01.296791Z","shell.execute_reply.started":"2025-12-02T17:09:55.610238Z","shell.execute_reply":"2025-12-02T17:10:01.295633Z"}},"outputs":[{"name":"stdout","text":"\nStarting training...\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_47/3904282550.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_47/3904282550.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, criterion, device, epoch_idx, num_epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtgt_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtgt_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1296\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3493\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3494\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3495\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3496\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":43},{"cell_type":"code","source":"def translate(model, sentence, max_len=60):\n    model.eval()\n\n    src = torch.tensor(encode(sentence)).unsqueeze(0).to(device)\n    tgt = torch.tensor([[sos_id]], dtype=torch.long).to(device)\n\n    for _ in range(max_len):\n        logits = model(src, tgt)\n        next_id = logits[:, -1].argmax(dim=-1).unsqueeze(1)\n        tgt = torch.cat([tgt, next_id], dim=1)\n        if next_id.item() == eos_id:\n            break\n    \n    tokens = tgt[0].tolist()[1:-1]   # remove sos & eos\n    return tokenizer.decode(tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:31:10.909950Z","iopub.status.idle":"2025-12-02T16:31:10.910219Z","shell.execute_reply.started":"2025-12-02T16:31:10.910099Z","shell.execute_reply":"2025-12-02T16:31:10.910110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(translate(model, \"I love machine learning.\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T16:31:10.910917Z","iopub.status.idle":"2025-12-02T16:31:10.911191Z","shell.execute_reply.started":"2025-12-02T16:31:10.911039Z","shell.execute_reply":"2025-12-02T16:31:10.911070Z"}},"outputs":[],"execution_count":null}]}