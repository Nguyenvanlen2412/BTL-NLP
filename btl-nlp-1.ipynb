{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd33e481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T12:14:14.867055Z",
     "iopub.status.busy": "2025-12-10T12:14:14.866360Z",
     "iopub.status.idle": "2025-12-10T12:14:16.205338Z",
     "shell.execute_reply": "2025-12-10T12:14:16.204542Z"
    },
    "papermill": {
     "duration": 1.345241,
     "end_time": "2025-12-10T12:14:16.206530",
     "exception": false,
     "start_time": "2025-12-10T12:14:14.861289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-dataset/public_test.en.txt\n",
      "/kaggle/input/nlp-dataset/train.en.txt\n",
      "/kaggle/input/nlp-dataset/train.vi.txt\n",
      "/kaggle/input/nlp-dataset/public_test.vi.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5c0702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T12:14:16.214265Z",
     "iopub.status.busy": "2025-12-10T12:14:16.213679Z",
     "iopub.status.idle": "2025-12-10T12:14:23.101234Z",
     "shell.execute_reply": "2025-12-10T12:14:23.100647Z"
    },
    "papermill": {
     "duration": 6.89266,
     "end_time": "2025-12-10T12:14:23.102633",
     "exception": false,
     "start_time": "2025-12-10T12:14:16.209973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#1. Load dataset\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ParallelTextDataset(Dataset):\n",
    "    def __init__(self, src_file, tgt_file):\n",
    "        self.data = []\n",
    "        \n",
    "        # Open both files together\n",
    "        with open(src_file, 'r', encoding='utf-8') as f1, \\\n",
    "             open(tgt_file, 'r', encoding='utf-8') as f2:\n",
    "            \n",
    "            # zip() pairs line 1 with line 1, line 2 with line 2...\n",
    "            for src_line, tgt_line in zip(f1, f2):\n",
    "                if src_line.strip() and tgt_line.strip(): # Skip empty lines\n",
    "                    self.data.append((src_line.strip(), tgt_line.strip()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Usage\n",
    "train_dataset = ParallelTextDataset(\n",
    "    \"/kaggle/input/nlp-dataset/train.en.txt\", \n",
    "    \"/kaggle/input/nlp-dataset/train.vi.txt\"\n",
    ")\n",
    "\n",
    "#train_dataset = train_dataset.shuffle(seed=42).select(range(400000))\n",
    "\n",
    "\n",
    "test_dataset = ParallelTextDataset(\n",
    "    \"/kaggle/input/nlp-dataset/public_test.en.txt\",\n",
    "    \"/kaggle/input/nlp-dataset/public_test.vi.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f05deb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T12:14:23.110551Z",
     "iopub.status.busy": "2025-12-10T12:14:23.109809Z",
     "iopub.status.idle": "2025-12-10T12:14:23.114560Z",
     "shell.execute_reply": "2025-12-10T12:14:23.113880Z"
    },
    "papermill": {
     "duration": 0.009625,
     "end_time": "2025-12-10T12:14:23.115540",
     "exception": false,
     "start_time": "2025-12-10T12:14:23.105915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: To evaluate clinical, subclinical symptoms of patients with otitis media with effusion and V.a at otorhinolaryngology department – Thai Nguyen national hospital\n",
      "Target: Nghiên cứu đặc điểm lâm sàng, cận lâm sàng bệnh nhân viêm tai ứ dịch trên viêm V.A tại Khoa Tai mũi họng - Bệnh viện Trung ương Thái Nguyên\n",
      "Source: Evaluate clinical, subclinical symptoms of patients with otittis media effusion and V a at otorhinolaryngology department - Thai Nguyên National Hospital.\n",
      "Target: Đánh giá đặc điểm lâm sàng, cận lâm sàng bệnh nhân viêm tai ứ dịch trên viêm V.a tại Khoa Tai mũi họng - Bệnh viện Trung ương Thái Nguyên.\n",
      "Source: There was a relation between vasodilatation and vaginal dysfunction.\n",
      "Target: Có sự liên quan giữa độ quá phát V.a với mức độ rối loạn chức năng vòi nhĩ.\n",
      "Source: Otittis media effusion on V a is a common disease in children.\n",
      "Target: Kết luận: Viêm tai ứ dịch trên viêm V.a là bệnh lý hay gặp ở lứa tuổi trẻ em.\n",
      "Source: Main symptoms are rhinitis, nasal congestion, tinnitus.\n",
      "Target: Triệu chứng cơ năng nổi bật là chảy mũi, ngạt mũi và ù tai.\n",
      "Source: Typical endoscopy are concave eardrum, yellow or bubbly.\n",
      "Target: Hình ảnh nội soi màng nhĩ điển hình là màng nhĩ lõm, màu vàng hoặc có bóng khí.\n",
      "Source: Tympanograms and audiograms help to dignosis exactly the level of vaginal dysfunction.\n",
      "Target: Nhĩ lượng đồ đánh giá khách quan mức độ rối loạn chức năng vòi.\n",
      "Source: Method: Prospective descriptive method with clinical intervention.\n",
      "Target: Phương pháp: Nghiên cứu mô tả từng trường hợp có can thiệp.\n",
      "Source: Results: Mean age was 5.1.\n",
      "Target: Kết quả: Tuổi trung bình 5,1.\n",
      "Source: Gender: male (52.6%), female (47.4%).\n",
      "Target: Tỉ lệ giới: Nam (52,6%), nữ (47,4%).\n",
      "500000\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "for i in range(10):\n",
    "    src, tgt = train_dataset[i]\n",
    "    print(f\"Source: {src}\")\n",
    "    print(f\"Target: {tgt}\")\n",
    "\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b0c85a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T12:14:23.122755Z",
     "iopub.status.busy": "2025-12-10T12:14:23.122347Z",
     "iopub.status.idle": "2025-12-10T12:14:38.502299Z",
     "shell.execute_reply": "2025-12-10T12:14:38.501459Z"
    },
    "papermill": {
     "duration": 15.384851,
     "end_time": "2025-12-10T12:14:38.503464",
     "exception": false,
     "start_time": "2025-12-10T12:14:23.118613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAINING ENGLISH BYTE-LEVEL BPE TOKENIZER ===\n",
      "\n",
      "\n",
      "\n",
      "✓ Saved English tokenizer (tokenizer_en.json)\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers, decoders\n",
    "\n",
    "print(\"\\n=== TRAINING ENGLISH BYTE-LEVEL BPE TOKENIZER ===\")\n",
    "\n",
    "tokenizer_en = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "\n",
    "# English: lowercase + strip accents\n",
    "tokenizer_en.normalizer = normalizers.Sequence([\n",
    "    normalizers.NFD(),\n",
    "    normalizers.Lowercase(),\n",
    "    normalizers.StripAccents(),\n",
    "])\n",
    "\n",
    "# Byte-level preserves whitespace (GPT2-style)\n",
    "tokenizer_en.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "tokenizer_en.decoder = decoders.ByteLevel()\n",
    "\n",
    "trainer_en = trainers.BpeTrainer(\n",
    "    vocab_size=24000,\n",
    "    special_tokens=[\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"],\n",
    ")\n",
    "\n",
    "def iter_english():\n",
    "    for src, _ in train_dataset:\n",
    "        yield src\n",
    "\n",
    "tokenizer_en.train_from_iterator(iter_english(), trainer_en)\n",
    "tokenizer_en.save(\"tokenizer_en.json\")\n",
    "\n",
    "print(\"✓ Saved English tokenizer (tokenizer_en.json)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2907ea6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T12:14:38.511335Z",
     "iopub.status.busy": "2025-12-10T12:14:38.511098Z",
     "iopub.status.idle": "2025-12-10T12:14:56.442440Z",
     "shell.execute_reply": "2025-12-10T12:14:56.441549Z"
    },
    "papermill": {
     "duration": 17.936959,
     "end_time": "2025-12-10T12:14:56.443937",
     "exception": false,
     "start_time": "2025-12-10T12:14:38.506978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAINING VIETNAMESE BYTE-LEVEL BPE TOKENIZER ===\n",
      "\n",
      "\n",
      "\n",
      "✓ Saved Vietnamese tokenizer (tokenizer_vi.json)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== TRAINING VIETNAMESE BYTE-LEVEL BPE TOKENIZER ===\")\n",
    "\n",
    "tokenizer_vi = Tokenizer(models.BPE(unk_token=\"<unk>\"))\n",
    "\n",
    "# Vietnamese: NFC (keeps tone marks stable)\n",
    "tokenizer_vi.normalizer = normalizers.NFC()\n",
    "\n",
    "# Byte-level for correct whitespace + multi-byte Unicode\n",
    "tokenizer_vi.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "tokenizer_vi.decoder = decoders.ByteLevel()\n",
    "\n",
    "trainer_vi = trainers.BpeTrainer(\n",
    "    vocab_size=28000, \n",
    "    special_tokens=[\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"],\n",
    ")\n",
    "\n",
    "def iter_vietnamese():\n",
    "    for _, tgt in train_dataset:\n",
    "        yield tgt\n",
    "\n",
    "tokenizer_vi.train_from_iterator(iter_vietnamese(), trainer_vi)\n",
    "tokenizer_vi.save(\"tokenizer_vi.json\")\n",
    "\n",
    "print(\"✓ Saved Vietnamese tokenizer (tokenizer_vi.json)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55996513",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T12:14:56.453384Z",
     "iopub.status.busy": "2025-12-10T12:14:56.452906Z",
     "iopub.status.idle": "2025-12-10T12:14:56.461203Z",
     "shell.execute_reply": "2025-12-10T12:14:56.460395Z"
    },
    "papermill": {
     "duration": 0.014215,
     "end_time": "2025-12-10T12:14:56.462429",
     "exception": false,
     "start_time": "2025-12-10T12:14:56.448214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, tokenizer_src, tokenizer_tgt, max_len=80):\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # Get IDs for special tokens\n",
    "        self.sos_id = tokenizer_src.token_to_id(\"<sos>\")\n",
    "        self.eos_id = tokenizer_src.token_to_id(\"<eos>\")\n",
    "        self.pad_id = tokenizer_src.token_to_id(\"<pad>\")\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        src_batch, tgt_batch = [], []\n",
    "        \n",
    "        for src_text, tgt_text in batch:\n",
    "            # 1. Encode\n",
    "            src_encoded = self.tokenizer_src.encode(src_text).ids\n",
    "            tgt_encoded = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "            # 2. Add <sos> and <eos> and Truncate if necessary\n",
    "            # For Source: usually need <sos>...<eos> (or just <eos> depending on arch)\n",
    "            src_ids = [self.sos_id] + src_encoded[:self.max_len-2] + [self.eos_id]\n",
    "            # For Target: <sos>...<eos>\n",
    "            tgt_ids = [self.sos_id] + tgt_encoded[:self.max_len-2] + [self.eos_id]\n",
    "\n",
    "            src_batch.append(torch.tensor(src_ids))\n",
    "            tgt_batch.append(torch.tensor(tgt_ids))\n",
    "\n",
    "        # 3. Pad\n",
    "        src_batch = pad_sequence(src_batch, padding_value=self.pad_id, batch_first=True)\n",
    "        tgt_batch = pad_sequence(tgt_batch, padding_value=self.pad_id, batch_first=True)\n",
    "\n",
    "        return src_batch, tgt_batch\n",
    "\n",
    "# Initialize Dataloader\n",
    "collate_fn = Collate(tokenizer_en, tokenizer_vi)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "389333e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T12:14:56.471673Z",
     "iopub.status.busy": "2025-12-10T12:14:56.471381Z",
     "iopub.status.idle": "2025-12-10T12:14:56.507291Z",
     "shell.execute_reply": "2025-12-10T12:14:56.506374Z"
    },
    "papermill": {
     "duration": 0.042195,
     "end_time": "2025-12-10T12:14:56.508469",
     "exception": false,
     "start_time": "2025-12-10T12:14:56.466274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# ============================================================================\n",
    "# 1. SCALED DOT-PRODUCT ATTENTION\n",
    "# ============================================================================\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (batch, n_heads, seq_len, d_k)\n",
    "            key: (batch, n_heads, seq_len, d_k)\n",
    "            value: (batch, n_heads, seq_len, d_v)\n",
    "            mask: (batch, 1, seq_len, seq_len) or (batch, 1, 1, seq_len)\n",
    "        Returns:\n",
    "            output: (batch, n_heads, seq_len, d_v)\n",
    "            attention_weights: (batch, n_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        d_k = query.size(-1)\n",
    "        \n",
    "        # (batch, n_heads, seq_len, seq_len)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        # Apply mask (set masked positions to large negative value)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Softmax over last dimension (key dimension)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # (batch, n_heads, seq_len, d_v)\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. MULTI-HEAD ATTENTION\n",
    "# ============================================================================\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention allows the model to attend to information \n",
    "    from different representation subspaces.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Split the last dimension into (n_heads, d_k)\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            (batch, n_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        return x.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        Combine heads back\n",
    "        Args:\n",
    "            x: (batch, n_heads, seq_len, d_k)\n",
    "        Returns:\n",
    "            (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, n_heads, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (batch, seq_len_q, d_model)\n",
    "            key: (batch, seq_len_k, d_model)\n",
    "            value: (batch, seq_len_v, d_model)\n",
    "            mask: (batch, 1, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        # Linear projections\n",
    "        Q = self.W_q(query)  # (batch, seq_len_q, d_model)\n",
    "        K = self.W_k(key)    # (batch, seq_len_k, d_model)\n",
    "        V = self.W_v(value)  # (batch, seq_len_v, d_model)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = self.split_heads(Q)  # (batch, n_heads, seq_len_q, d_k)\n",
    "        K = self.split_heads(K)  # (batch, n_heads, seq_len_k, d_k)\n",
    "        V = self.split_heads(V)  # (batch, n_heads, seq_len_v, d_k)\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_output, attn_weights = self.attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads\n",
    "        attn_output = self.combine_heads(attn_output)  # (batch, seq_len_q, d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. POSITION-WISE FEED-FORWARD NETWORK\n",
    "# ============================================================================\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "    Two linear transformations with ReLU activation in between.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, d_ff) -> (batch, seq_len, d_model)\n",
    "        return self.fc2(self.dropout(F.relu(self.fc1(x))))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. POSITIONAL ENCODING\n",
    "# ============================================================================\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=100, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 5. ENCODER LAYER\n",
    "# ============================================================================\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single encoder layer:\n",
    "    1. Multi-head self-attention\n",
    "    2. Add & Norm\n",
    "    3. Feed-forward\n",
    "    4. Add & Norm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "            mask: (batch, 1, 1, seq_len) - padding mask\n",
    "        \"\"\"\n",
    "        # Self-attention with residual connection\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout2(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 6. DECODER LAYER\n",
    "# ============================================================================\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single decoder layer:\n",
    "    1. Masked multi-head self-attention\n",
    "    2. Add & Norm\n",
    "    3. Multi-head cross-attention (with encoder output)\n",
    "    4. Add & Norm\n",
    "    5. Feed-forward\n",
    "    6. Add & Norm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, tgt_seq_len, d_model)\n",
    "            enc_output: (batch, src_seq_len, d_model)\n",
    "            src_mask: (batch, 1, 1, src_seq_len) - encoder padding mask\n",
    "            tgt_mask: (batch, 1, tgt_seq_len, tgt_seq_len) - decoder causal + padding mask\n",
    "        \"\"\"\n",
    "        # Masked self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # Cross-attention with encoder output\n",
    "        attn_output, _ = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout2(attn_output))\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout3(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 7. ENCODER\n",
    "# ============================================================================\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of N encoder layers\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_layers, dropout=0.1, max_len=100, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(d_model)\n",
    "    \n",
    "    def forward(self, src, src_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (batch, src_seq_len)\n",
    "            src_mask: (batch, 1, 1, src_seq_len)\n",
    "        Returns:\n",
    "            (batch, src_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = self.embedding(src) * self.scale\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 8. DECODER\n",
    "# ============================================================================\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stack of N decoder layers\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_layers, dropout=0.1, max_len=100, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(d_model)\n",
    "    \n",
    "    def forward(self, tgt, enc_output, src_mask=None, tgt_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt: (batch, tgt_seq_len)\n",
    "            enc_output: (batch, src_seq_len, d_model)\n",
    "            src_mask: (batch, 1, 1, src_seq_len)\n",
    "            tgt_mask: (batch, 1, tgt_seq_len, tgt_seq_len)\n",
    "        Returns:\n",
    "            (batch, tgt_seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = self.embedding(tgt) * self.scale\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 9. COMPLETE TRANSFORMER MODEL\n",
    "# ============================================================================\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer for sequence-to-sequence translation\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        tgt_vocab_size,\n",
    "        d_model=384,\n",
    "        n_heads=6,\n",
    "        n_encoder_layers=4,\n",
    "        n_decoder_layers=4,\n",
    "        d_ff=1536,\n",
    "        dropout=0.1,\n",
    "        max_len=100,\n",
    "        pad_idx=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size, d_model, n_heads, d_ff, \n",
    "            n_encoder_layers, dropout, max_len, pad_idx\n",
    "        )\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "            tgt_vocab_size, d_model, n_heads, d_ff, \n",
    "            n_decoder_layers, dropout, max_len, pad_idx\n",
    "        )\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        # --- OPTIMIZATION START ---\n",
    "        # 1. Create the full-sized mask once (max_len x max_len)\n",
    "        #    This creates the upper triangular matrix (future positions)\n",
    "        mask = torch.triu(torch.ones(max_len, max_len), diagonal=1).bool()\n",
    "        \n",
    "        # 2. Invert it (True = visible, False = hidden) so it matches your logic\n",
    "        #    (Your original code used ~mask, so we store the inverted version)\n",
    "        causal_mask = ~mask \n",
    "        \n",
    "        # 3. Register as a buffer\n",
    "        #    'causal_mask' becomes accessible as self.causal_mask\n",
    "        #    It will automatically move to GPU when you do model.to(device)\n",
    "        self.register_buffer('causal_mask', causal_mask)\n",
    "        # --- OPTIMIZATION END ---\n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "    \n",
    "    def _init_parameters(self):\n",
    "        \"\"\"Initialize parameters with Xavier uniform\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def create_padding_mask(self, seq):\n",
    "        \"\"\"\n",
    "        Create padding mask\n",
    "        Args:\n",
    "            seq: (batch, seq_len)\n",
    "        Returns:\n",
    "            (batch, 1, 1, seq_len)\n",
    "        \"\"\"\n",
    "        # 1 where token is not pad, 0 where it is pad\n",
    "        mask = (seq != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return mask\n",
    "    \n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (batch, src_seq_len)\n",
    "            tgt: (batch, tgt_seq_len)\n",
    "        Returns:\n",
    "            (batch, tgt_seq_len, tgt_vocab_size)\n",
    "        \"\"\"\n",
    "        # Get the current sequence length (e.g., 32, 50, etc.)\n",
    "        seq_len = tgt.size(1)\n",
    "        \n",
    "        # Create masks\n",
    "        src_mask = self.create_padding_mask(src)\n",
    "        tgt_padding_mask = self.create_padding_mask(tgt)\n",
    "        \n",
    "        # --- OPTIMIZATION START ---\n",
    "        # Slice the pre-made buffer. \n",
    "        # No .to(device) needed! It's already there.\n",
    "        # Reshape to (1, 1, seq_len, seq_len) to match attention logic\n",
    "        tgt_causal_mask = self.causal_mask[:seq_len, :seq_len].unsqueeze(0).unsqueeze(0)\n",
    "        # --- OPTIMIZATION END ---\n",
    "        \n",
    "        # Combine target masks\n",
    "        tgt_mask = tgt_padding_mask & tgt_causal_mask\n",
    "        \n",
    "        # Encode\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        \n",
    "        # Decode\n",
    "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        output = self.fc_out(dec_output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def encode(self, src):\n",
    "        \"\"\"Encode source sequence\"\"\"\n",
    "        src_mask = self.create_padding_mask(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "    def decode(self, tgt, enc_output, src_mask):\n",
    "        \"\"\"Decode one step\"\"\"\n",
    "        tgt_mask = self.create_causal_mask(tgt.size(1)).to(tgt.device)\n",
    "        tgt_padding_mask = self.create_padding_mask(tgt)\n",
    "        tgt_mask = tgt_padding_mask & tgt_mask\n",
    "        \n",
    "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "        return self.fc_out(dec_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a61b000a",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-12-10T12:14:56.517323Z",
     "iopub.status.busy": "2025-12-10T12:14:56.517105Z",
     "iopub.status.idle": "2025-12-10T12:16:05.884138Z",
     "shell.execute_reply": "2025-12-10T12:16:05.883487Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 69.37318,
     "end_time": "2025-12-10T12:16:05.885530",
     "exception": false,
     "start_time": "2025-12-10T12:14:56.512350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.8.2)\r\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (25.0)\r\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.15.2)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\r\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics) (2.4.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.10.0)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>1.20.0->torchmetrics) (2024.2.0)\r\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics\n",
    "\n",
    "from torchmetrics.text import BLEUScore\n",
    "\n",
    "def calculate_bleu(model, dataloader, tokenizer_src, tokenizer_tgt, device, max_batches=None):\n",
    "    \"\"\"\n",
    "    Calculates BLEU score on the validation set.\n",
    "    \n",
    "    Args:\n",
    "        max_batches (int): Limit the number of batches to speed up evaluation (optional)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    metric = BLEUScore()\n",
    "    \n",
    "    # Store predictions and references\n",
    "    preds = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (src, tgt) in enumerate(tqdm(dataloader, desc=\"Calculating BLEU\")):\n",
    "            # Optional: Stop early if it takes too long\n",
    "            if max_batches and batch_idx >= max_batches:\n",
    "                break\n",
    "\n",
    "            src = src.to(device)\n",
    "            \n",
    "            # 1. Iterate over the batch (Since your greedy_decode is single-sample)\n",
    "            # Optimization note: Writing a batch_greedy_decode function would be faster,\n",
    "            # but looping is safer with your current code structure.\n",
    "            for i in range(src.size(0)):\n",
    "                # Get single source sentence tensor: (1, seq_len)\n",
    "                src_tensor = src[i].unsqueeze(0)\n",
    "                \n",
    "                # Decode (Generate prediction)\n",
    "                # Note: We use your existing greedy_decode function\n",
    "                pred_tokens = greedy_decode(model, src_tensor, tokenizer_tgt, device)\n",
    "                \n",
    "                # Convert IDs to Text (remove <sos>, <eos>, <pad>)\n",
    "                pred_text = tokenizer_tgt.decode(pred_tokens, skip_special_tokens=True)\n",
    "                \n",
    "                # Get Reference Text\n",
    "                # tgt[i] is the reference tensor\n",
    "                ref_tokens = tgt[i].tolist()\n",
    "                ref_text = tokenizer_tgt.decode(ref_tokens, skip_special_tokens=True)\n",
    "                \n",
    "                preds.append(pred_text)\n",
    "                # BLEU expects a list of references for each prediction (e.g. [[ref1], [ref2]])\n",
    "                targets.append([ref_text])\n",
    "                \n",
    "    # Compute Final BLEU Score\n",
    "    score = metric(preds, targets)\n",
    "    return score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1d80f28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T12:16:05.925189Z",
     "iopub.status.busy": "2025-12-10T12:16:05.924771Z",
     "iopub.status.idle": "2025-12-10T12:16:05.946413Z",
     "shell.execute_reply": "2025-12-10T12:16:05.945838Z"
    },
    "papermill": {
     "duration": 0.042824,
     "end_time": "2025-12-10T12:16:05.947470",
     "exception": false,
     "start_time": "2025-12-10T12:16:05.904646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LEARNING RATE SCHEDULER (Transformer Warmup)\n",
    "# ============================================================================\n",
    "class TransformerLRScheduler:\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.step_num = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.step_num += 1\n",
    "        lr = self._get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    def _get_lr(self):\n",
    "        step = max(self.step_num, 1)\n",
    "        return (self.d_model ** -0.5) * min(\n",
    "            step ** -0.5, \n",
    "            step * (self.warmup_steps ** -1.5)\n",
    "        )\n",
    "    \n",
    "    def get_last_lr(self):\n",
    "        return [self._get_lr()]\n",
    "\n",
    "# ============================================================================\n",
    "# 2. TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "def train_epoch(model, dataloader, criterion, optimizer, scheduler, device, grad_clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch_idx, (src, tgt) in enumerate(pbar):\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        \n",
    "        # Target input (without last token) and output (without first token)\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src, tgt_input)\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        # output: (batch, seq_len, vocab_size) -> (batch * seq_len, vocab_size)\n",
    "        output = output.reshape(-1, output.size(-1))\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, tgt_output)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        total_loss += loss.item()\n",
    "        # total_tokens += (tgt_output != model.pad_idx).sum().item() # Optional token counting\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'avg_loss': total_loss / (batch_idx + 1),\n",
    "            'lr': scheduler.get_last_lr()[0]\n",
    "        })\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. VALIDATION FUNCTION\n",
    "# ============================================================================\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src, tgt in tqdm(dataloader, desc=\"Validation\"):\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            \n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "            \n",
    "            output = model(src, tgt_input)\n",
    "            \n",
    "            output = output.reshape(-1, output.size(-1))\n",
    "            tgt_output = tgt_output.reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt_output)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# ============================================================================\n",
    "# 4. GREEDY DECODING (FOR INFERENCE)\n",
    "# ============================================================================\n",
    "def greedy_decode(model, src, tokenizer_tgt, device, max_len=128):\n",
    "    model.eval()\n",
    "    src = src.to(device)\n",
    "    \n",
    "    # Encode source\n",
    "    with torch.no_grad():\n",
    "        enc_output = model.encode(src)\n",
    "        src_mask = model.create_padding_mask(src)\n",
    "    \n",
    "    # Start with <sos> token\n",
    "    sos_idx = tokenizer_tgt.token_to_id(\"<sos>\")\n",
    "    eos_idx = tokenizer_tgt.token_to_id(\"<eos>\")\n",
    "    \n",
    "    tgt_tokens = [sos_idx]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        tgt = torch.LongTensor([tgt_tokens]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model.decode(tgt, enc_output, src_mask)\n",
    "        \n",
    "        # Get the last token prediction\n",
    "        next_token_logits = output[0, -1, :]\n",
    "        next_token = next_token_logits.argmax().item()\n",
    "        \n",
    "        tgt_tokens.append(next_token)\n",
    "        \n",
    "        if next_token == eos_idx:\n",
    "            break\n",
    "    \n",
    "    return tgt_tokens\n",
    "\n",
    "# ============================================================================\n",
    "# 5. TRANSLATION FUNCTION\n",
    "# ============================================================================\n",
    "def translate(model, src_text, tokenizer_src, tokenizer_tgt, device, max_len=128):\n",
    "    model.eval()\n",
    "    \n",
    "    sos_idx = tokenizer_src.token_to_id(\"<sos>\")\n",
    "    eos_idx = tokenizer_src.token_to_id(\"<eos>\")\n",
    "    \n",
    "    # Tokenize source\n",
    "    src_ids = [sos_idx] + tokenizer_src.encode(src_text).ids + [eos_idx]\n",
    "    src = torch.LongTensor([src_ids])\n",
    "    \n",
    "    tgt_tokens = greedy_decode(model, src, tokenizer_tgt, device, max_len)\n",
    "    \n",
    "    # Convert to text (remove <sos> and <eos>)\n",
    "    tgt_text = tokenizer_tgt.decode(tgt_tokens[1:-1])\n",
    "    \n",
    "    return tgt_text\n",
    "\n",
    "# ============================================================================\n",
    "# 6. HELPER: BLEU SCORE PLACEHOLDER (ADDED TO FIX MISSING FUNCTION ERROR)\n",
    "# ============================================================================\n",
    "def calculate_bleu(model, dataloader, tokenizer_src, tokenizer_tgt, device, max_batches=None):\n",
    "    \"\"\"\n",
    "    Placeholder for BLEU calculation to prevent NameError.\n",
    "    You should implement the actual BLEU logic using torchtext or sacrebleu.\n",
    "    \"\"\"\n",
    "    print(\"\\n[NOTE] calculate_bleu is a placeholder. Logic needs to be implemented.\")\n",
    "    return 0.0\n",
    "\n",
    "# ============================================================================\n",
    "# 7. MAIN TRAINING LOOP\n",
    "# ============================================================================\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    device,\n",
    "    num_epochs=25,\n",
    "    d_model=384,\n",
    "    warmup_steps=6000,\n",
    "    grad_clip=1.0,\n",
    "    save_dir=\"checkpoints\",\n",
    "    print_every=1,\n",
    "    bleu_max_batches=None, \n",
    "    early_stopping=True,\n",
    "    patience=5\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Note: Ensure your model has a 'pad_idx' attribute or pass it explicitly\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=model.pad_idx, label_smoothing=0.1)\n",
    "    optimizer = Adam(model.parameters(), lr=1.0, betas=(0.9, 0.98), eps=1e-9)\n",
    "    scheduler = TransformerLRScheduler(optimizer, d_model, warmup_steps)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    print(f\"Starting training on {device}\")\n",
    "    print(\"BLEU calculation disabled during training. Will run once at the end.\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # 1. TRAIN\n",
    "        train_loss = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, scheduler, device, grad_clip\n",
    "        )\n",
    "\n",
    "        # 2. VALIDATE\n",
    "        val_loss = None\n",
    "        if val_loader is not None:\n",
    "            val_loss = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        # 3. RECORD & PRINT\n",
    "        history['train_loss'].append(train_loss)\n",
    "        if val_loss is not None:\n",
    "            history['val_loss'].append(val_loss)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "        if val_loss is not None:\n",
    "            print(f\"  Val Loss:   {val_loss:.4f}\")\n",
    "\n",
    "        # 4. SAVE CHECKPOINTS\n",
    "        if val_loss is not None and val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improve_epochs = 0\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, f\"{save_dir}/best_model.pt\")\n",
    "            print(f\"  ✓ Saved best model (val_loss={val_loss:.4f})\")\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            print(f\"  No improvement for {no_improve_epochs} epoch(s).\")\n",
    "\n",
    "        # EARLY STOPPING\n",
    "        if early_stopping and no_improve_epochs >= patience:\n",
    "            print(\"\\n🛑 Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "        # 5. SAMPLE TRANSLATIONS\n",
    "        if epoch % print_every == 0:\n",
    "            pass # Add your custom sample printing logic here\n",
    "\n",
    "    # ========================================================================\n",
    "    # FINAL EVALUATION\n",
    "    # ========================================================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TRAINING FINISHED. CALCULATING FINAL BLEU...\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Load best model\n",
    "    best_checkpoint_path = f\"{save_dir}/best_model.pt\"\n",
    "    if os.path.exists(best_checkpoint_path):\n",
    "        print(f\"Loading best checkpoint from {best_checkpoint_path}...\")\n",
    "        checkpoint = torch.load(best_checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Calculate BLEU\n",
    "    if val_loader is not None:\n",
    "        final_bleu = calculate_bleu(\n",
    "            model,\n",
    "            val_loader,\n",
    "            tokenizer_src,\n",
    "            tokenizer_tgt,\n",
    "            device,\n",
    "            max_batches=bleu_max_batches \n",
    "        )\n",
    "        print(f\"\\n🏆 Final BLEU Score (Best Model): {final_bleu:.4f}\")\n",
    "        history['final_bleu'] = final_bleu\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c48594e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T12:16:05.985021Z",
     "iopub.status.busy": "2025-12-10T12:16:05.984775Z",
     "iopub.status.idle": "2025-12-10T12:16:07.520254Z",
     "shell.execute_reply": "2025-12-10T12:16:07.519335Z"
    },
    "papermill": {
     "duration": 1.555711,
     "end_time": "2025-12-10T12:16:07.521571",
     "exception": false,
     "start_time": "2025-12-10T12:16:05.965860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully!\n",
      "Input source shape: torch.Size([64, 20])\n",
      "Input target shape: torch.Size([64, 25])\n",
      "Output shape: torch.Size([64, 25, 28000])\n",
      "Expected output shape: (batch_size=64, tgt_seq_len=25, tgt_vocab_size=28000)\n",
      "\n",
      "Total parameters: 47,312,224\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "src_vocab_size = 24000\n",
    "tgt_vocab_size = 28000\n",
    "d_model = 384\n",
    "n_heads = 6\n",
    "n_encoder_layers = 4\n",
    "n_decoder_layers = 4\n",
    "d_ff = 1536\n",
    "dropout = 0.1\n",
    "pad_idx = 0\n",
    "\n",
    "# Create model\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_heads=n_heads,\n",
    "    n_encoder_layers=n_encoder_layers,\n",
    "    n_decoder_layers=n_decoder_layers,\n",
    "    d_ff=d_ff,\n",
    "    dropout=dropout,\n",
    "    pad_idx=pad_idx\n",
    ")\n",
    "\n",
    "# Example input\n",
    "batch_size = 64\n",
    "src_seq_len = 20\n",
    "tgt_seq_len = 25\n",
    "\n",
    "src = torch.randint(1, src_vocab_size, (batch_size, src_seq_len))\n",
    "tgt = torch.randint(1, tgt_vocab_size, (batch_size, tgt_seq_len))\n",
    "\n",
    "# Forward pass\n",
    "output = model(src, tgt)\n",
    "\n",
    "print(f\"Model created successfully!\")\n",
    "print(f\"Input source shape: {src.shape}\")\n",
    "print(f\"Input target shape: {tgt.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expected output shape: (batch_size={batch_size}, tgt_seq_len={tgt_seq_len}, tgt_vocab_size={tgt_vocab_size})\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c71341e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T12:16:07.561801Z",
     "iopub.status.busy": "2025-12-10T12:16:07.561061Z",
     "iopub.status.idle": "2025-12-10T12:16:07.626514Z",
     "shell.execute_reply": "2025-12-10T12:16:07.625422Z"
    },
    "papermill": {
     "duration": 0.086214,
     "end_time": "2025-12-10T12:16:07.627969",
     "exception": false,
     "start_time": "2025-12-10T12:16:07.541755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 INSPECTING DATA ALIGNMENT...\n",
      "\n",
      "--- Example 1 ---\n",
      "SRC (Input):   chromosomal disorders of embryos in the 5th day with mother’s age in in-vitro fertilization\n",
      "TGT (Label):   Liên quan giữa rối loạn nhiễm sắc thể ở phôi ngày 5 với tuổi mẹ trong thụ tinh trong ống nghiệm\n",
      "SRC IDs:      [   2 6669  892  165 8743  159  158  335  271  564]...\n",
      "TGT IDs:      [   2 3226  409  671  685  606  489 1294  298  299]...\n",
      "\n",
      "--- Example 2 ---\n",
      "SRC (Input):   patients and method: case series on all patients diagnosed mediastinal tumors at children's hospital 1 and 2 in the period of time mentioned above.\n",
      "TGT (Label):   Đối tượng và phương pháp nghiên cứu: Mô tả các bệnh nhi dưới 15 tuổi được chẩn đoán u trung thất tại bệnh viện Nhi Đồng 1 và Nhi Đồng 2 trong thời gian trên.\n",
      "SRC IDs:      [   2  226  177  656   29  978 2712  256  608  226]...\n",
      "TGT IDs:      [   2  761  708  216  530  404  420  328   29 1278]...\n",
      "\n",
      "--- Example 3 ---\n",
      "SRC (Input):   - number of metastatic lymph nodes was 2.77 ± 3.8.\n",
      "TGT (Label):   - Số hạch di căn trung bình 2,77 ± 3,8 (0-14) hạch.\n",
      "SRC IDs:      [   2  298 1207  165 3979 1230 3361  254  219   17]...\n",
      "TGT IDs:      [   2  484 1600 1233  803 1123  485  511  291   15]...\n"
     ]
    }
   ],
   "source": [
    "def check_data_alignment(dataloader, tokenizer_src, tokenizer_tgt):\n",
    "    print(\"🔎 INSPECTING DATA ALIGNMENT...\")\n",
    "    \n",
    "    # Get one batch\n",
    "    src_batch, tgt_batch = next(iter(dataloader))\n",
    "    \n",
    "    # Check the first 3 examples in the batch\n",
    "    for i in range(3):\n",
    "        print(f\"\\n--- Example {i+1} ---\")\n",
    "        \n",
    "        # Get raw IDs\n",
    "        src_ids = src_batch[i].numpy()\n",
    "        tgt_ids = tgt_batch[i].numpy()\n",
    "        \n",
    "        # Decode back to text (removing padding for readability)\n",
    "        # Note: We skip special tokens like <sos>, <eos>, <pad> to see the raw content\n",
    "        src_text = tokenizer_src.decode(src_ids, skip_special_tokens=True)\n",
    "        tgt_text = tokenizer_tgt.decode(tgt_ids, skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"SRC (Input):  {src_text}\")\n",
    "        print(f\"TGT (Label):  {tgt_text}\")\n",
    "        print(f\"SRC IDs:      {src_ids[:10]}...\") # Print first 10 IDs\n",
    "        print(f\"TGT IDs:      {tgt_ids[:10]}...\")\n",
    "\n",
    "# Run this before your training loop\n",
    "check_data_alignment(train_loader, tokenizer_en, tokenizer_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b332c8e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T12:16:07.671814Z",
     "iopub.status.busy": "2025-12-10T12:16:07.671077Z",
     "iopub.status.idle": "2025-12-10T17:05:33.794672Z",
     "shell.execute_reply": "2025-12-10T17:05:33.793961Z"
    },
    "papermill": {
     "duration": 17366.146277,
     "end_time": "2025-12-10T17:05:33.796160",
     "exception": false,
     "start_time": "2025-12-10T12:16:07.649883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Source vocab size: 24000\n",
      "Target vocab size: 28000\n",
      "Starting training on cuda\n",
      "BLEU calculation disabled during training. Will run once at the end.\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7813/7813 [26:11<00:00,  4.97it/s, loss=7.46, avg_loss=26.5, lr=0.000577]\n",
      "Validation: 100%|██████████| 47/47 [00:03<00:00, 13.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/25\n",
      "  Train Loss: 26.4573\n",
      "  Val Loss:   7.5052\n",
      "  ✓ Saved best model (val_loss=7.5052)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7813/7813 [26:14<00:00,  4.96it/s, loss=7.39, avg_loss=7.46, lr=0.000408]\n",
      "Validation: 100%|██████████| 47/47 [00:03<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/25\n",
      "  Train Loss: 7.4577\n",
      "  Val Loss:   7.4360\n",
      "  ✓ Saved best model (val_loss=7.4360)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7813/7813 [26:12<00:00,  4.97it/s, loss=7.28, avg_loss=7.44, lr=0.000333]\n",
      "Validation: 100%|██████████| 47/47 [00:03<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/25\n",
      "  Train Loss: 7.4436\n",
      "  Val Loss:   7.4251\n",
      "  ✓ Saved best model (val_loss=7.4251)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7813/7813 [26:15<00:00,  4.96it/s, loss=7.4, avg_loss=7.44, lr=0.000289]\n",
      "Validation: 100%|██████████| 47/47 [00:03<00:00, 13.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/25\n",
      "  Train Loss: 7.4378\n",
      "  Val Loss:   7.4252\n",
      "  No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7813/7813 [26:13<00:00,  4.97it/s, loss=7.33, avg_loss=7.43, lr=0.000258]\n",
      "Validation: 100%|██████████| 47/47 [00:03<00:00, 13.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/25\n",
      "  Train Loss: 7.4340\n",
      "  Val Loss:   7.4331\n",
      "  No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7813/7813 [26:12<00:00,  4.97it/s, loss=7.36, avg_loss=7.43, lr=0.000236]\n",
      "Validation: 100%|██████████| 47/47 [00:03<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/25\n",
      "  Train Loss: 7.4311\n",
      "  Val Loss:   7.4234\n",
      "  ✓ Saved best model (val_loss=7.4234)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7813/7813 [26:11<00:00,  4.97it/s, loss=7.45, avg_loss=7.43, lr=0.000218]\n",
      "Validation: 100%|██████████| 47/47 [00:03<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/25\n",
      "  Train Loss: 7.4288\n",
      "  Val Loss:   7.4337\n",
      "  No improvement for 1 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7813/7813 [26:14<00:00,  4.96it/s, loss=7.35, avg_loss=7.43, lr=0.000204]\n",
      "Validation: 100%|██████████| 47/47 [00:03<00:00, 13.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/25\n",
      "  Train Loss: 7.4263\n",
      "  Val Loss:   7.4907\n",
      "  No improvement for 2 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7813/7813 [26:14<00:00,  4.96it/s, loss=7.41, avg_loss=7.42, lr=0.000192]\n",
      "Validation: 100%|██████████| 47/47 [00:03<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/25\n",
      "  Train Loss: 7.4242\n",
      "  Val Loss:   7.4705\n",
      "  No improvement for 3 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7813/7813 [26:19<00:00,  4.95it/s, loss=7.25, avg_loss=7.42, lr=0.000183]\n",
      "Validation: 100%|██████████| 47/47 [00:03<00:00, 13.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/25\n",
      "  Train Loss: 7.4221\n",
      "  Val Loss:   7.6527\n",
      "  No improvement for 4 epoch(s).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7813/7813 [26:19<00:00,  4.95it/s, loss=7.41, avg_loss=7.42, lr=0.000174]\n",
      "Validation: 100%|██████████| 47/47 [00:03<00:00, 13.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/25\n",
      "  Train Loss: 7.4186\n",
      "  Val Loss:   35.6030\n",
      "  No improvement for 5 epoch(s).\n",
      "\n",
      "🛑 Early stopping triggered!\n",
      "\n",
      "======================================================================\n",
      "TRAINING FINISHED. CALCULATING FINAL BLEU...\n",
      "======================================================================\n",
      "Loading best checkpoint from checkpoints/best_model.pt...\n",
      "\n",
      "[NOTE] calculate_bleu is a placeholder. Logic needs to be implemented.\n",
      "\n",
      "🏆 Final BLEU Score (Best Model): 0.0000\n",
      "\n",
      "Training completed!\n",
      "Saved training history plot to 'training_history.png'\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete training script using your data and tokenizers\n",
    "\"\"\"\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load tokenizers (assuming you have them from your code)\n",
    "from tokenizers import Tokenizer\n",
    "tokenizer_en = Tokenizer.from_file(\"tokenizer_en.json\")\n",
    "tokenizer_vi = Tokenizer.from_file(\"tokenizer_vi.json\")\n",
    "\n",
    "# Get vocab sizes\n",
    "src_vocab_size = tokenizer_en.get_vocab_size()\n",
    "tgt_vocab_size = tokenizer_vi.get_vocab_size()\n",
    "pad_idx = tokenizer_en.token_to_id(\"<pad>\")\n",
    "\n",
    "print(f\"Source vocab size: {src_vocab_size}\")\n",
    "print(f\"Target vocab size: {tgt_vocab_size}\")\n",
    "\n",
    "# Initialize model\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=384,\n",
    "    n_heads=6,\n",
    "    n_encoder_layers=4,\n",
    "    n_decoder_layers=4,\n",
    "    d_ff=1536,\n",
    "    dropout=0.1,\n",
    "    pad_idx=pad_idx\n",
    ").to(device)\n",
    "\n",
    "# Create data loaders (using your Collate class and datasets)\n",
    "from torch.utils.data import DataLoader\n",
    "# Assuming you have: train_dataset, test_dataset, collate_fn\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    tokenizer_src=tokenizer_en,\n",
    "    tokenizer_tgt=tokenizer_vi,\n",
    "    device=device,\n",
    "    num_epochs=25,\n",
    "    d_model=384,\n",
    "    warmup_steps=6000,\n",
    "    grad_clip=1.0,\n",
    "    save_dir=\"checkpoints\",\n",
    "    print_every=3\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "\n",
    "# Plot training history (optional)\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    if history['val_loss']:\n",
    "        plt.plot(history['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_history.png')\n",
    "    print(\"Saved training history plot to 'training_history.png'\")\n",
    "except ImportError:\n",
    "    print(\"Matplotlib not available for plotting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b9f89",
   "metadata": {
    "papermill": {
     "duration": 7.51845,
     "end_time": "2025-12-10T17:05:49.092306",
     "exception": false,
     "start_time": "2025-12-10T17:05:41.573856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8957868,
     "sourceId": 14072576,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17508.50742,
   "end_time": "2025-12-10T17:05:59.849070",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-10T12:14:11.341650",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
